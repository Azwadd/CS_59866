{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3- Feature Engineering for Classification\n",
    "\n",
    "Goal: Train the best classifier possible for Heart Disease Prediction while trying various feature engineering techniques and learning to run experiments to find the best possible model.\n",
    "\n",
    "\n",
    "### Feature Preprocessing\n",
    "For each categorical features implement two feature representations:\n",
    "1. OneHot Encoding: For example, transform `X_train['Sex']` with values `M or F` into two features `X_Train['Sex_M']`, `X_Train['Sex_F']` with values `0 or 1`\n",
    "2. Target Encoding: For example, transform `X_train['Sex']` with values `M or F` into a feature `X_Train['Sex-TargetEncoded']` with value equal to the average rate of heart disease of `M and F` respectively.\n",
    "\n",
    "Please implement these yourself, but you can check against sklearn.preprocessing implementations for correctness.\n",
    "\n",
    "The set of categorical features is:\n",
    "`categorical_features = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']`\n",
    "\n",
    "\n",
    "For numerical features implement two feature normalisations:\n",
    "1. Standard Scaler (scale numeric values to have zero mean and unit variance). For example `X_train['Age-Scaled'] = (X_train['Age'] - mean) / standard_deviation`\n",
    "2. MinMax Normalization (subtract min and divide by max - min). For example `X_train['Age-MinMax'] = (X_train['Age'] - min) / (max - min)`\n",
    "\n",
    "Please implement these yourself but you can check against the sklearn.preprocessing implementations.\n",
    "\n",
    "The set of numeric features is:\n",
    "`numeric_features = [\"Age\", \"RestingBP\", \"Cholesterol\", \"FastingBS\", \"MaxHR\", \"Oldpeak\"]`\n",
    "    \n",
    "Note, any feature preprocessing parameters (like mean or standard deviation) should be calculated on training data only.\n",
    "\n",
    "### Feature Engineering:\n",
    "Create at least 5 custom features as functions of other features:\n",
    "\n",
    "Some ideas:\n",
    "  - A binary feature representing High Heart Rate `Custom-BinaryHighMaxHR`\n",
    "  - A bucketized categorical feature of Oldpeak or of MaxHR called `Oldpeak`\n",
    "  - A feature cross of a bucketized version of OldPeak and MaxHR for example `HighOldPeak_X_LowMaxHR`\n",
    "  \n",
    "### Feature Experiments:\n",
    "Please run at least this set of experiments, but try others as you see fit.\n",
    "1. All features with no scaling on numeric features and OneHot for categorical. No custom features.\n",
    "2. All features using StandardScaler for numeric and OneHot for categorical. No custom features.\n",
    "3. All features using StandardScaler for numeric and TargetEncoding for categorical. No custom features.\n",
    "4. All features using MinMax-Normalization for numeric and OneHot for categorical. No custom features.\n",
    "5. The kitchen sink: Include everything to try to get the best performance possible.\n",
    "6. Only custom features. How good can you get with your own custom feature set?\n",
    "7. Only categorical features using one of the encodings.\n",
    "8. Only numerical features using one of the encodings.\n",
    "\n",
    "\n",
    "### Model Training\n",
    "Model training Data:\n",
    "Separate data into a training set with 80% of the samples and a test set with 20%. \n",
    "`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)`. Note that ideally we would also have a validation set, but since this dataset is fairly small, we will just do train/test splits. Please use the same random state as above so we have comparable performance across the class.\n",
    "\n",
    "### Models\n",
    "Please run all experiments using at least 3 different models\n",
    "1. Logistic regression using `sklearn.linear_model.LogisticRegression`. Extra credit of +2 if you use your own gradient descent implementation from Homework 2.\n",
    "2. Decision Trees: using `sklearn.tree.DecisionTreeClassifier`. Hyperparameter here is the depth of the tree.\n",
    "3. Neural Net classifier using `sklearn.neural_network.MLPClassifier`. Hyperparameters here include the shape of the network and the learning rate. Consider using the 'adam' solver as constructor argument, 'relu' activation fuctions (which is the default), and try with a shape of network, but start small with a network of size `(4,1)`, that is 4 hidden units in one layer or `(3,2)` which includes 2 hidden layers of 3 units each.\n",
    "4. At least 1 other classifier. Some ideas include SVM classifiers or Gradient Boosted Decision trees which are all available in sklearn as well\n",
    "\n",
    "\n",
    "### Question 1: Run Experiments (15 points)\n",
    "\n",
    "We have many varying parameters. Often in ML we run multiple experiments to find the best result.\n",
    "- at least 8 different feature setups, maybe more\n",
    "- at least 4 different model algorithms\n",
    "- some number of hyperparemeters, let's say we try at least 10 for each model\n",
    "\n",
    "This gives us a minimum of 320 different experiments. To easily keep track of these, let's create a spreadsheet to track the experiment results. \n",
    "\n",
    "Create a .csv file with the following columns tracking your experiments. Make sure this .csv can be loaded into Google sheets so I can grade it. Upload the .csv separately.\n",
    "\n",
    "1. Experiment Name string. For example `Experiment1-LogReg-OneHot-Custom`. This name be anything.\n",
    "2. List of Feature Included in Model separated by semicolons. Please name these according to this scheme: `<FeatureName>-<Preprocessing>-<Value>` for example `MaxHR-StandardScaler` or `RestingECG-OneHot-Normal`. For custom features name them `Custom-<Name>` for example: `Custom-BucketizedOldpeak`. As an example, a row in this columns might look like `MaxHR; MaxHR-StandardScaler; Custom-BinaryHighHeartRate; ...`. If your features are in your dataframe, you can create this list with `';'.join(X_train.columns)`.\n",
    "3. Model Type (MyLogisticRegression or DecisionTree or anything else you want to try)\n",
    "4. Epochs (for logistic regression)\n",
    "5. Hyperparameters used in your model, for example depth of tree or size of neural network\n",
    "6. Training Accuracy (w/ threshold of 0.5)\n",
    "7. Testing Accuracy (w/ threshold of 0.5)\n",
    "8. Training PR-AUC (Area under the precision/recall curve)\n",
    "9. Testing PR-AUC (Area under the precision/recall curve)\n",
    "10. Training Precision (w/ threshold of 0.5)\n",
    "11. Testing Precision (w/ threshold of 0.5)\n",
    "12. Training Recall (w/ threshold of 0.5)\n",
    "13. Testing Recall (w/ threshold of 0.5)\n",
    "\n",
    "Extra Credit (2 points): Use Weights And Biases (https://wandb.ai/) to track your experiments as an alternative to a spreadsheet (https://towardsdatascience.com/introduction-to-weight-biases-track-and-visualize-your-machine-learning-experiments-in-3-lines-9c9553b0f99d)\n",
    "\n",
    "\n",
    "### Question 2: Analyze Logistic Regression Experiments (4 points)\n",
    "\n",
    "- Question 2.1: What is the best set of features and parameters for Logistic Regression in terms of Test PR-AUC. Is this the same as the best in terms of accuracy? Discuss why you think this experiment showed the best results.\n",
    "- Question 2.2: If features are well normalized, we can get a sense of feature importance by looking at the absolute value of the weights of each feature. Print out the highest 5 weights by absolute value. Which features are these? Discuss what this set of 5 features has on the model.\n",
    "- Question 2.3: Train a model with just those top 5 features. What is the Test Accuracy and PR-AUC?\n",
    "- Question 2.4: Describe the custom features you added. Why did you pick these?\n",
    "\n",
    "### Question 3: Analyze Other Experiments (4 points)\n",
    "\n",
    "- Question 3.1: What is the best set of features and parameters for Decision Tree classifier in terms of Test PR-AUC.  Discuss why you think this experiment showed the best results.\n",
    "- Question 3.2: What is the best set of features and parameters for Neural Net classifier in terms of Test PR-AUC.  Discuss why you think this experiment showed the best results.\n",
    "- Question 3.3: What is the best set of features and parameters for another chosen model in terms of Test PR-AUC.  Discuss why you think this experiment showed the best results.\n",
    "\n",
    "### Question 4: Achieving particular peformance charactaristics (2 points)\n",
    "- Question 4.1: From your trained models, can you produce a classifier that has around 90% recall on the test set? How? What is the precision of this model?\n",
    "- Question 4.2: From your trained models, can you produce a classifier that has around 90% precision on the test set? How? What is the recall of this model?\n",
    "\n",
    "### Question 5: Taking a step back (2 points)\n",
    "- Question 5.1: What is your best performing model over all experiments? Describe why you think this was the best? What is the PR-AUC and Accuracy of this model? Feel free to share your results on discord, the best model in the class will receive extra credit of 3 points!\n",
    "- Question 5.2: Discuss your thoughts on this process? What surprised you? What was hard or tedious? What did you learn?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, precision_recall_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load data from our csv\n",
    "df = pd.read_csv('../../Data/heart.csv')\n",
    "\n",
    "# Select out Binary and Categorical Features\n",
    "numeric_features = [\"Age\", \"RestingBP\", \"Cholesterol\", \"FastingBS\", \"MaxHR\", \"Oldpeak\"]\n",
    "categorical_features = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "label_feature = \"HeartDisease\"\n",
    "X = df[numeric_features + categorical_features]\n",
    "y = df[label_feature]\n",
    "\n",
    "# Create Train / Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Keeping track of custom features list\n",
    "custom_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.65122615803815\n",
      "9.364289800106517\n"
     ]
    }
   ],
   "source": [
    "# Examples of Pandas operations you can use:\n",
    "# Calculate mean of a column:\n",
    "print(X_train['Age'].mean())\n",
    "# Calculate standard deviation of a column:\n",
    "print(X_train['Age'].std())\n",
    "\n",
    "# Example of creating a custom new feature column, in this case a binary feature indicating age > 50\n",
    "X_train_old = X_train.copy()\n",
    "X_test_old = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex_M</th>\n",
       "      <th>Sex_F</th>\n",
       "      <th>ChestPainType_NAP</th>\n",
       "      <th>ChestPainType_ASY</th>\n",
       "      <th>ChestPainType_TA</th>\n",
       "      <th>ChestPainType_ATA</th>\n",
       "      <th>RestingECG_Normal</th>\n",
       "      <th>RestingECG_LVH</th>\n",
       "      <th>RestingECG_ST</th>\n",
       "      <th>ExerciseAngina_N</th>\n",
       "      <th>ExerciseAngina_Y</th>\n",
       "      <th>ST_Slope_Down</th>\n",
       "      <th>ST_Slope_Up</th>\n",
       "      <th>ST_Slope_Flat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>734 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sex_M  Sex_F  ChestPainType_NAP  ChestPainType_ASY  ChestPainType_TA  \\\n",
       "795      1      0                  1                  0                 0   \n",
       "25       1      0                  1                  0                 0   \n",
       "84       1      0                  0                  1                 0   \n",
       "10       0      1                  1                  0                 0   \n",
       "344      1      0                  0                  1                 0   \n",
       "..     ...    ...                ...                ...               ...   \n",
       "106      0      1                  0                  1                 0   \n",
       "270      1      0                  0                  1                 0   \n",
       "860      1      0                  0                  1                 0   \n",
       "435      1      0                  0                  1                 0   \n",
       "102      0      1                  0                  1                 0   \n",
       "\n",
       "     ChestPainType_ATA  RestingECG_Normal  RestingECG_LVH  RestingECG_ST  \\\n",
       "795                  0                  1               0              0   \n",
       "25                   0                  1               0              0   \n",
       "84                   0                  1               0              0   \n",
       "10                   0                  1               0              0   \n",
       "344                  0                  1               0              0   \n",
       "..                 ...                ...             ...            ...   \n",
       "106                  0                  0               0              1   \n",
       "270                  0                  1               0              0   \n",
       "860                  0                  1               0              0   \n",
       "435                  0                  0               0              1   \n",
       "102                  0                  1               0              0   \n",
       "\n",
       "     ExerciseAngina_N  ExerciseAngina_Y  ST_Slope_Down  ST_Slope_Up  \\\n",
       "795                 1                 0              1            0   \n",
       "25                  1                 0              0            1   \n",
       "84                  0                 1              0            0   \n",
       "10                  1                 0              0            1   \n",
       "344                 1                 0              0            0   \n",
       "..                ...               ...            ...          ...   \n",
       "106                 1                 0              0            1   \n",
       "270                 1                 0              0            1   \n",
       "860                 0                 1              0            1   \n",
       "435                 0                 1              0            1   \n",
       "102                 1                 0              0            0   \n",
       "\n",
       "     ST_Slope_Flat  \n",
       "795              0  \n",
       "25               0  \n",
       "84               1  \n",
       "10               0  \n",
       "344              1  \n",
       "..             ...  \n",
       "106              0  \n",
       "270              0  \n",
       "860              0  \n",
       "435              0  \n",
       "102              1  \n",
       "\n",
       "[734 rows x 14 columns]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Preprocessing\n",
    "# 1. OneHot Encoding: For example, transform X_train['Sex'] with values M or F into two features X_Train['Sex_M'], X_Train['Sex_F'] with values 0 or 1\n",
    "def oneHotEncoder(features, categories):\n",
    "    feature_list = []\n",
    "    for i in categories:\n",
    "        for j in features[i].unique():\n",
    "            features[i + \"_\" + j] = (X[i] == j).astype(int)\n",
    "            feature_list.append(i + \"_\" + j)\n",
    "    return feature_list\n",
    "\n",
    "# Outputs all the categorical columns with oneHot Encoding\n",
    "OneHot_Encoding_Feature_List = oneHotEncoder(X_train, categorical_features) # X_train has been fitted with new columns\n",
    "oneHotEncoder(X_test, categorical_features) # X_test has been fitted with new columns\n",
    "X_train[OneHot_Encoding_Feature_List] # Showing the new fitted columns in X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex-TargetEncoded</th>\n",
       "      <th>ChestPainType-TargetEncoded</th>\n",
       "      <th>RestingECG-TargetEncoded</th>\n",
       "      <th>ExerciseAngina-TargetEncoded</th>\n",
       "      <th>ST_Slope-TargetEncoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>0.632042</td>\n",
       "      <td>0.364198</td>\n",
       "      <td>0.509009</td>\n",
       "      <td>0.327146</td>\n",
       "      <td>0.770833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.632042</td>\n",
       "      <td>0.364198</td>\n",
       "      <td>0.509009</td>\n",
       "      <td>0.327146</td>\n",
       "      <td>0.175896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.632042</td>\n",
       "      <td>0.770574</td>\n",
       "      <td>0.509009</td>\n",
       "      <td>0.858086</td>\n",
       "      <td>0.817942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.253012</td>\n",
       "      <td>0.364198</td>\n",
       "      <td>0.509009</td>\n",
       "      <td>0.327146</td>\n",
       "      <td>0.175896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>0.632042</td>\n",
       "      <td>0.770574</td>\n",
       "      <td>0.509009</td>\n",
       "      <td>0.327146</td>\n",
       "      <td>0.817942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.253012</td>\n",
       "      <td>0.770574</td>\n",
       "      <td>0.645390</td>\n",
       "      <td>0.327146</td>\n",
       "      <td>0.175896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0.632042</td>\n",
       "      <td>0.770574</td>\n",
       "      <td>0.509009</td>\n",
       "      <td>0.327146</td>\n",
       "      <td>0.175896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0.632042</td>\n",
       "      <td>0.770574</td>\n",
       "      <td>0.509009</td>\n",
       "      <td>0.858086</td>\n",
       "      <td>0.175896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>0.632042</td>\n",
       "      <td>0.770574</td>\n",
       "      <td>0.645390</td>\n",
       "      <td>0.858086</td>\n",
       "      <td>0.175896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.253012</td>\n",
       "      <td>0.770574</td>\n",
       "      <td>0.509009</td>\n",
       "      <td>0.327146</td>\n",
       "      <td>0.817942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>734 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sex-TargetEncoded  ChestPainType-TargetEncoded  RestingECG-TargetEncoded  \\\n",
       "795           0.632042                     0.364198                  0.509009   \n",
       "25            0.632042                     0.364198                  0.509009   \n",
       "84            0.632042                     0.770574                  0.509009   \n",
       "10            0.253012                     0.364198                  0.509009   \n",
       "344           0.632042                     0.770574                  0.509009   \n",
       "..                 ...                          ...                       ...   \n",
       "106           0.253012                     0.770574                  0.645390   \n",
       "270           0.632042                     0.770574                  0.509009   \n",
       "860           0.632042                     0.770574                  0.509009   \n",
       "435           0.632042                     0.770574                  0.645390   \n",
       "102           0.253012                     0.770574                  0.509009   \n",
       "\n",
       "     ExerciseAngina-TargetEncoded  ST_Slope-TargetEncoded  \n",
       "795                      0.327146                0.770833  \n",
       "25                       0.327146                0.175896  \n",
       "84                       0.858086                0.817942  \n",
       "10                       0.327146                0.175896  \n",
       "344                      0.327146                0.817942  \n",
       "..                            ...                     ...  \n",
       "106                      0.327146                0.175896  \n",
       "270                      0.327146                0.175896  \n",
       "860                      0.858086                0.175896  \n",
       "435                      0.858086                0.175896  \n",
       "102                      0.327146                0.817942  \n",
       "\n",
       "[734 rows x 5 columns]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Preprocessing\n",
    "# 2. Target Encoding: For example, transform X_train['Sex'] with values M or F into a feature X_Train['Sex-TargetEncoded'] with value equal to the average rate of heart disease of M and F respectively.\n",
    "def targetEncoder(features, label, categories):\n",
    "    feature_list = []\n",
    "    for i in categories:\n",
    "        features[i+\"-TargetEncoded\"] = features[i].map(pd.concat([features, label], axis=1).groupby(i)[label_feature].mean())\n",
    "        feature_list.append(i+\"-TargetEncoded\")\n",
    "    return feature_list\n",
    "\n",
    "targetEncoded_feature_List = targetEncoder(X_train, y_train, categorical_features) # X_train has been fitted with new columns\n",
    "targetEncoder(X_test, y_test, categorical_features)  # X_test has been fitted with new columns\n",
    "X_train[targetEncoded_feature_List] # Showing the new fitted columns in X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age-Scaled</th>\n",
       "      <th>RestingBP-Scaled</th>\n",
       "      <th>Cholesterol-Scaled</th>\n",
       "      <th>FastingBS-Scaled</th>\n",
       "      <th>MaxHR-Scaled</th>\n",
       "      <th>Oldpeak-Scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>-1.244219</td>\n",
       "      <td>-0.708502</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>1.841354</td>\n",
       "      <td>2.282796</td>\n",
       "      <td>-0.096995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.884951</td>\n",
       "      <td>-0.166172</td>\n",
       "      <td>0.086087</td>\n",
       "      <td>-0.542339</td>\n",
       "      <td>1.651116</td>\n",
       "      <td>-0.835717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.250822</td>\n",
       "      <td>0.918489</td>\n",
       "      <td>0.123050</td>\n",
       "      <td>1.841354</td>\n",
       "      <td>-0.441327</td>\n",
       "      <td>0.087685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.778162</td>\n",
       "      <td>-0.166172</td>\n",
       "      <td>0.104569</td>\n",
       "      <td>-0.542339</td>\n",
       "      <td>0.229834</td>\n",
       "      <td>-0.835717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>-0.283121</td>\n",
       "      <td>-0.708502</td>\n",
       "      <td>-1.845220</td>\n",
       "      <td>1.841354</td>\n",
       "      <td>-1.270407</td>\n",
       "      <td>-0.835717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>-0.603487</td>\n",
       "      <td>-0.708502</td>\n",
       "      <td>0.501919</td>\n",
       "      <td>-0.542339</td>\n",
       "      <td>-1.033527</td>\n",
       "      <td>-0.835717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>-0.923853</td>\n",
       "      <td>-0.708502</td>\n",
       "      <td>0.233938</td>\n",
       "      <td>-0.542339</td>\n",
       "      <td>0.150874</td>\n",
       "      <td>-0.835717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0.677977</td>\n",
       "      <td>-0.166172</td>\n",
       "      <td>0.492678</td>\n",
       "      <td>-0.542339</td>\n",
       "      <td>0.308794</td>\n",
       "      <td>0.457046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>0.677977</td>\n",
       "      <td>1.026955</td>\n",
       "      <td>-1.845220</td>\n",
       "      <td>-0.542339</td>\n",
       "      <td>-0.717687</td>\n",
       "      <td>-0.835717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-1.457796</td>\n",
       "      <td>0.918489</td>\n",
       "      <td>1.777136</td>\n",
       "      <td>-0.542339</td>\n",
       "      <td>-0.243926</td>\n",
       "      <td>1.011087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>734 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age-Scaled  RestingBP-Scaled  Cholesterol-Scaled  FastingBS-Scaled  \\\n",
       "795   -1.244219         -0.708502            0.372549          1.841354   \n",
       "25    -1.884951         -0.166172            0.086087         -0.542339   \n",
       "84     0.250822          0.918489            0.123050          1.841354   \n",
       "10    -1.778162         -0.166172            0.104569         -0.542339   \n",
       "344   -0.283121         -0.708502           -1.845220          1.841354   \n",
       "..          ...               ...                 ...               ...   \n",
       "106   -0.603487         -0.708502            0.501919         -0.542339   \n",
       "270   -0.923853         -0.708502            0.233938         -0.542339   \n",
       "860    0.677977         -0.166172            0.492678         -0.542339   \n",
       "435    0.677977          1.026955           -1.845220         -0.542339   \n",
       "102   -1.457796          0.918489            1.777136         -0.542339   \n",
       "\n",
       "     MaxHR-Scaled  Oldpeak-Scaled  \n",
       "795      2.282796       -0.096995  \n",
       "25       1.651116       -0.835717  \n",
       "84      -0.441327        0.087685  \n",
       "10       0.229834       -0.835717  \n",
       "344     -1.270407       -0.835717  \n",
       "..            ...             ...  \n",
       "106     -1.033527       -0.835717  \n",
       "270      0.150874       -0.835717  \n",
       "860      0.308794        0.457046  \n",
       "435     -0.717687       -0.835717  \n",
       "102     -0.243926        1.011087  \n",
       "\n",
       "[734 rows x 6 columns]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Preprocessing\n",
    "# 1. Standard Scaler (scale numeric values to have zero mean and unit variance). For example X_train['Age-Scaled'] = (X_train['Age'] - mean) / standard_deviation\n",
    "def standardScaler(features, categories):\n",
    "    features_list = []\n",
    "    for i in categories:\n",
    "        features[i+\"-Scaled\"] = (features[i] - features[i].mean()) / features[i].std()\n",
    "        features_list.append(i+\"-Scaled\")\n",
    "    return features_list\n",
    "\n",
    "standardScaler_Feature_List = standardScaler(X_train, numeric_features) # X_train has been fitted with new columns\n",
    "standardScaler(X_test, numeric_features) # X_test has been fitted with new columns\n",
    "X_train[standardScaler_Feature_List] # Showing the new fitted columns in X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age-MinMax</th>\n",
       "      <th>RestingBP-MinMax</th>\n",
       "      <th>Cholesterol-MinMax</th>\n",
       "      <th>FastingBS-MinMax</th>\n",
       "      <th>MaxHR-MinMax</th>\n",
       "      <th>Oldpeak-MinMax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.398010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.943662</td>\n",
       "      <td>0.386364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.346600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.830986</td>\n",
       "      <td>0.295455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.353234</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.457746</td>\n",
       "      <td>0.409091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.349917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.577465</td>\n",
       "      <td>0.295455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.309859</td>\n",
       "      <td>0.295455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.421227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.352113</td>\n",
       "      <td>0.295455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.373134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.563380</td>\n",
       "      <td>0.295455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.419569</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.591549</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.408451</td>\n",
       "      <td>0.295455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.650083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.492958</td>\n",
       "      <td>0.522727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>734 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age-MinMax  RestingBP-MinMax  Cholesterol-MinMax  FastingBS-MinMax  \\\n",
       "795    0.270833              0.60            0.398010               1.0   \n",
       "25     0.145833              0.65            0.346600               0.0   \n",
       "84     0.562500              0.75            0.353234               1.0   \n",
       "10     0.166667              0.65            0.349917               0.0   \n",
       "344    0.458333              0.60            0.000000               1.0   \n",
       "..          ...               ...                 ...               ...   \n",
       "106    0.395833              0.60            0.421227               0.0   \n",
       "270    0.333333              0.60            0.373134               0.0   \n",
       "860    0.645833              0.65            0.419569               0.0   \n",
       "435    0.645833              0.76            0.000000               0.0   \n",
       "102    0.229167              0.75            0.650083               0.0   \n",
       "\n",
       "     MaxHR-MinMax  Oldpeak-MinMax  \n",
       "795      0.943662        0.386364  \n",
       "25       0.830986        0.295455  \n",
       "84       0.457746        0.409091  \n",
       "10       0.577465        0.295455  \n",
       "344      0.309859        0.295455  \n",
       "..            ...             ...  \n",
       "106      0.352113        0.295455  \n",
       "270      0.563380        0.295455  \n",
       "860      0.591549        0.454545  \n",
       "435      0.408451        0.295455  \n",
       "102      0.492958        0.522727  \n",
       "\n",
       "[734 rows x 6 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Preprocessing\n",
    "# 2. MinMax Normalization (subtract min and divide by max - min). For example X_train['Age-MinMax'] = (X_train['Age'] - min) / (max - min)\n",
    "def minMax(features, categories):\n",
    "    features_list = []\n",
    "    for i in categories:\n",
    "        features[i+\"-MinMax\"] = (features[i] - features[i].min()) / (features[i].max() - features[i].min())\n",
    "        features_list.append(i+\"-MinMax\")\n",
    "    return features_list\n",
    "\n",
    "minMax_feature_list = minMax(X_train, numeric_features) # X_train has been fitted with new columns\n",
    "minMax(X_test, numeric_features) # X_test has been fitted with new columns\n",
    "X_train[minMax_feature_list] # Showing the new fitted columns in X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>BinaryHighMaxHR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>194</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>142</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>142</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     MaxHR  BinaryHighMaxHR\n",
       "795    194                1\n",
       "25     178                1\n",
       "84     125                0\n",
       "10     142                1\n",
       "344    104                0\n",
       "254     96                0\n",
       "398    122                0\n",
       "244    103                0\n",
       "621    142                1\n",
       "118    185                1"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Engineering --> 1st custom feature\n",
    "# A binary feature representing High Heart Rate BinaryHighMaxHR\n",
    "def custom_BinaryFeature(features, old_feature, new_feature):\n",
    "    feature_mean = features[0][old_feature].mean()\n",
    "    features[0][new_feature] = (features[0][old_feature] >= feature_mean).apply(int)\n",
    "    features[1][new_feature] = (features[1][old_feature] >= feature_mean).apply(int)\n",
    "    return new_feature\n",
    "\n",
    "custom_features.append(custom_BinaryFeature((X_train, X_test), \"MaxHR\", \"BinaryHighMaxHR\")) \n",
    "X_train[[\"MaxHR\", \"BinaryHighMaxHR\"]].head(10)  # Showing the new fitted columns in X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max 6.2 \tMean: 0.9050408719346048 \tMin -2.6 \tStd 1.0829519487461572\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bucket-LowOldpeak</th>\n",
       "      <th>Bucket-MidOldpeak</th>\n",
       "      <th>Bucket-HighOldpeak</th>\n",
       "      <th>Oldpeak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Bucket-LowOldpeak  Bucket-MidOldpeak  Bucket-HighOldpeak  Oldpeak\n",
       "795                  0                  1                   0      0.8\n",
       "25                   1                  0                   0      0.0\n",
       "84                   0                  1                   0      1.0\n",
       "10                   1                  0                   0      0.0\n",
       "344                  1                  0                   0      0.0\n",
       "254                  0                  0                   1      2.0\n",
       "398                  0                  1                   0      1.0\n",
       "244                  0                  1                   0      1.0\n",
       "621                  0                  1                   0      0.6\n",
       "118                  1                  0                   0      0.0"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Engineering --> 2nd custom feature \n",
    "# A bucketized categorical feature of Oldpeak called bucketized_Oldpeak\n",
    "def bucketized_feature(features, feature, intervals):\n",
    "    for f in features:\n",
    "        f[\"Bucket-Low\" + feature] = (f[feature] <= intervals[0]).apply(int)\n",
    "        f[\"Bucket-Mid\" + feature] = ((f[feature] <  intervals[1]) & (f[feature] >  intervals[0])).apply(int)\n",
    "        f[\"Bucket-High\" + feature] = (f[feature] >= intervals[1]).apply(int)\n",
    "    return [\"Bucket-Low\" + feature, \"Bucket-Mid\" + feature, \"Bucket-High\" + feature]\n",
    "\n",
    "\n",
    "print(\"Max\", X_train[\"Oldpeak\"].max(), \"\\tMean:\", X_train[\"Oldpeak\"].mean(), \"\\tMin\", X_train[\"Oldpeak\"].min(), \"\\tStd\", X_train[\"Oldpeak\"].std())\n",
    "bucketized_Oldpeak_list = bucketized_feature((X_train, X_test), \"Oldpeak\", (0, 2))\n",
    "for i in bucketized_Oldpeak_list: custom_features.append(i)\n",
    "X_train[bucketized_Oldpeak_list + [\"Oldpeak\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max 202 \tMean: 136.17847411444143 \tMin 60 \tStd 25.329253934908074\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bucket-LowMaxHR</th>\n",
       "      <th>Bucket-MidMaxHR</th>\n",
       "      <th>Bucket-HighMaxHR</th>\n",
       "      <th>MaxHR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Bucket-LowMaxHR  Bucket-MidMaxHR  Bucket-HighMaxHR  MaxHR\n",
       "795                0                0                 1    194\n",
       "25                 0                0                 1    178\n",
       "84                 0                1                 0    125\n",
       "10                 0                1                 0    142\n",
       "344                1                0                 0    104\n",
       "254                1                0                 0     96\n",
       "398                0                1                 0    122\n",
       "244                1                0                 0    103\n",
       "621                0                1                 0    142\n",
       "118                0                0                 1    185"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Engineering --> 3rd custom feature\n",
    "# A bucketized categorical feature of MaxHR called bucketized_MaxHR\n",
    "print(\"Max\", X_train[\"MaxHR\"].max(), \"\\tMean:\", X_train[\"MaxHR\"].mean(), \"\\tMin\", X_train[\"MaxHR\"].min(), \"\\tStd\", X_train[\"MaxHR\"].std())\n",
    "bucketized_MaxHR_list = bucketized_feature((X_train, X_test), \"MaxHR\", (110, 160))\n",
    "for i in bucketized_MaxHR_list: custom_features.append(i)\n",
    "X_train[bucketized_MaxHR_list + [\"MaxHR\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>Bucket-HighMaxHR</th>\n",
       "      <th>HighOldpeak_X_LowMaxHR</th>\n",
       "      <th>midOldpeak_X_MidMaxHR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>0.8</td>\n",
       "      <td>194</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1.0</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>0.0</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>2.0</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>1.0</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>1.0</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>0.6</td>\n",
       "      <td>142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.0</td>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Oldpeak  MaxHR  Bucket-HighMaxHR  HighOldpeak_X_LowMaxHR  \\\n",
       "795      0.8    194                 1                       0   \n",
       "25       0.0    178                 1                       0   \n",
       "84       1.0    125                 0                       0   \n",
       "10       0.0    142                 0                       0   \n",
       "344      0.0    104                 0                       0   \n",
       "254      2.0     96                 0                       1   \n",
       "398      1.0    122                 0                       0   \n",
       "244      1.0    103                 0                       0   \n",
       "621      0.6    142                 0                       0   \n",
       "118      0.0    185                 1                       0   \n",
       "\n",
       "     midOldpeak_X_MidMaxHR  \n",
       "795                      0  \n",
       "25                       0  \n",
       "84                       1  \n",
       "10                       0  \n",
       "344                      0  \n",
       "254                      0  \n",
       "398                      1  \n",
       "244                      0  \n",
       "621                      1  \n",
       "118                      0  "
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Engineering --> 4th custom feature\n",
    "# A feature cross of a bucketized version of OldPeak and MaxHR for example HighOldpeak_X_LowMaxHR\n",
    "def cross_feature(features, feature_one, feature_two, new_feature):\n",
    "    custom_feature = []\n",
    "    for f in features:\n",
    "        f[new_feature] = ((f[feature_one] == 1) & (f[feature_two] == 1)).astype(int)\n",
    "    return new_feature\n",
    "\n",
    "custom_features.append(cross_feature((X_train, X_test), \"Bucket-HighOldpeak\", \"Bucket-LowMaxHR\", \"HighOldpeak_X_LowMaxHR\"))\n",
    "custom_features.append(cross_feature((X_train, X_test), \"Bucket-MidOldpeak\", \"Bucket-MidMaxHR\", \"midOldpeak_X_MidMaxHR\"))\n",
    "custom_features.append(cross_feature((X_train, X_test), \"Bucket-LowOldpeak\", \"Bucket-HighMaxHR\", \"lowOldpeak_X_HighMaxHR\"))\n",
    "X_train[[\"Oldpeak\", \"MaxHR\"] + custom_features[len(custom_features)-4:len(custom_features)-1]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max 77 \tMean: 53.65122615803815 \tMin 29 \tStd 9.364289800106517\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Bucket-LowAge</th>\n",
       "      <th>Bucket-MidAge</th>\n",
       "      <th>Bucket-HighAge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age  Bucket-LowAge  Bucket-MidAge  Bucket-HighAge\n",
       "795   42              1              0               0\n",
       "25    36              1              0               0\n",
       "84    56              0              1               0\n",
       "10    37              1              0               0\n",
       "344   51              0              1               0\n",
       "254   55              0              1               0\n",
       "398   52              0              1               0\n",
       "244   48              0              1               0\n",
       "621   56              0              1               0\n",
       "118   35              1              0               0"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Engineering --> 5th custom feature\n",
    "# A feature cross of a bucketized version of OldPeak and MaxHR for example HighOldpeak_X_LowMaxHR\n",
    "print(\"Max\", X_train[\"Age\"].max(), \"\\tMean:\", X_train[\"Age\"].mean(), \"\\tMin\", X_train[\"Age\"].min(), \"\\tStd\", X_train[\"Age\"].std())\n",
    "bucketized_Age_list = bucketized_feature((X_train, X_test), \"Age\", (45, 60))\n",
    "for i in bucketized_Age_list: custom_features.append(i)\n",
    "X_train[[\"Age\"] + bucketized_Age_list].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Feature Experiments: All features with no scaling on numeric features and OneHot for categorical. No custom features.\n",
    "# Experiment 1 - All features with no scaling on numeric features and OneHot for categorical. No custom features.\n",
    "exp1_X_train = X_train[numeric_features + OneHot_Encoding_Feature_List]\n",
    "exp1_X_test = X_test[numeric_features + OneHot_Encoding_Feature_List]\n",
    "\n",
    "# Experiment 2 - All features using StandardScaler for numeric and OneHot for categorical. No custom features.\n",
    "exp2_X_train = X_train[OneHot_Encoding_Feature_List + standardScaler_Feature_List]\n",
    "exp2_X_test = X_test[OneHot_Encoding_Feature_List + standardScaler_Feature_List]\n",
    "\n",
    "# Experiment 3 - All features using StandardScaler for numeric and TargetEncoding for categorical. No custom features.\n",
    "exp3_X_train = X_train[standardScaler_Feature_List +  targetEncoded_feature_List]\n",
    "exp3_X_test = X_test[standardScaler_Feature_List + targetEncoded_feature_List]\n",
    "\n",
    "# Experiment 4 - All features using MinMax-Normalization for numeric and OneHot for categorical. No custom features.\n",
    "exp4_X_train = X_train[minMax_feature_list +  OneHot_Encoding_Feature_List]\n",
    "exp4_X_test = X_test[minMax_feature_list + OneHot_Encoding_Feature_List]\n",
    "\n",
    "# Experiment 5 - The kitchen sink: Include everything to try to get the best performance possible.\n",
    "exp5_X_train = X_train[numeric_features + standardScaler_Feature_List +  minMax_feature_list + OneHot_Encoding_Feature_List + targetEncoded_feature_List]\n",
    "exp5_X_test = X_test[numeric_features + standardScaler_Feature_List +  minMax_feature_list + OneHot_Encoding_Feature_List + targetEncoded_feature_List]\n",
    "\n",
    "# Experiment 6 - Only custom features. How good can you get with your own custom feature set?\n",
    "exp6_X_train = X_train[custom_features]\n",
    "exp6_X_test = X_test[custom_features]\n",
    "\n",
    "# Experiment 7 - Only categorical features using one of the encodings.\n",
    "exp7_X_train = X_train[OneHot_Encoding_Feature_List]\n",
    "exp7_X_test = X_test[OneHot_Encoding_Feature_List]\n",
    "\n",
    "# Experiment 8 - Only numerical features using one of the encodings.\n",
    "exp8_X_train = X_train[standardScaler_Feature_List]\n",
    "exp8_X_test = X_test[standardScaler_Feature_List]\n",
    "\n",
    "# Experiment 9 -  Only categorical features using one of the encodings.\n",
    "exp9_X_train = X_train[targetEncoded_feature_List]\n",
    "exp9_X_test = X_test[targetEncoded_feature_List]\n",
    "\n",
    "# Experiment 10 - Only numerical features using one of the encodings.\n",
    "exp10_X_train = X_train[minMax_feature_list]\n",
    "exp10_X_test = X_test[minMax_feature_list]\n",
    "\n",
    "experiments_train = [exp1_X_train, exp2_X_train, exp3_X_train, exp4_X_train, exp5_X_train, exp6_X_train, exp7_X_train, exp8_X_train, exp9_X_train, exp10_X_train]\n",
    "experiments_test = [exp1_X_test, exp2_X_test, exp3_X_test, exp4_X_test, exp5_X_test, exp6_X_test, exp7_X_test, exp8_X_test, exp9_X_test, exp10_X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models + extra\n",
    "def results(model, hyperparameters, features_train, labels_train, features_test, labels_test):\n",
    "    print(hyperparameters)\n",
    "    training_accuracy = accuracy_score(y_train, model.predict(features_train))\n",
    "    testing_accuracy = accuracy_score(y_test, model.predict(features_test))\n",
    "    print(f\"Training Accuracy: {training_accuracy}\")\n",
    "    print(f\"Testing Accuracy: {testing_accuracy}\")\n",
    "    training_precisions, training_recalls, training_thresholds = precision_recall_curve(y_train, model.predict(features_train))\n",
    "    training_pr_auc = auc( training_recalls,  training_precisions)\n",
    "    print(f\"Training PR_AUC: {training_pr_auc}\")\n",
    "    testing_precisions, testing_recalls, testing_thresholds = precision_recall_curve(y_test, model.predict(features_test))\n",
    "    testing_pr_auc = auc(testing_recalls, testing_precisions)\n",
    "    print(f\"Testing PR_AUC: {testing_pr_auc}\")\n",
    "    print(f\"Training Precision: {training_precisions}\")\n",
    "    print(f\"Testing Precision: {testing_precisions}\")\n",
    "    print(f\"Training Recall: {training_recalls}\")\n",
    "    print(f\"Testing Recall: {testing_recalls}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticReg(features, labels):\n",
    "    models, hyperparameters = [], []\n",
    "    hyperparameters.append(\"LogisticRegression: solver=lbfgs\")\n",
    "    models.append(LogisticRegression(max_iter=500, solver=\"lbfgs\").fit(features, labels))\n",
    "    hyperparameters.append(\"LogisticRegression: solver=newton-cg\")\n",
    "    models.append(LogisticRegression(max_iter=500, solver=\"newton-cg\").fit(features, labels))\n",
    "    hyperparameters.append(\"LogisticRegression: solver=liblinear\")\n",
    "    models.append(LogisticRegression(max_iter=500, solver=\"liblinear\").fit(features, labels))\n",
    "    # Seems like Newton-cg has highest score -> some models need a ridiculous amount of iterations  for convergence\n",
    "\n",
    "    # Solver newton-cg has only 2 penalties\n",
    "    hyperparameters.append(\"LogisticRegression: solver=lbfgs & penalty=l2\")\n",
    "    models.append(LogisticRegression(max_iter=500, solver=\"newton-cg\", penalty=\"l2\").fit(features, labels))\n",
    "    hyperparameters.append(\"LogisticRegression: solver=lbfgs & penalty=none\")\n",
    "    models.append(LogisticRegression(max_iter=500, solver=\"newton-cg\", penalty=\"none\").fit(features, labels))\n",
    "    # penalty as none and l2 seem to give the same results\n",
    "    return models, hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTree(features, labels):\n",
    "    models, hyperparameters = [], []\n",
    "    hyperparameters.append(\"DecisionTreeClassifier: criterion=gini\")\n",
    "    models.append(DecisionTreeClassifier(criterion=\"gini\").fit(features, labels))\n",
    "    hyperparameters.append(\"DecisionTreeClassifier: criterion=entropy\")\n",
    "    models.append(DecisionTreeClassifier(criterion=\"entropy\").fit(features, labels))\n",
    "    # criterion=entropy had higher scores \n",
    "\n",
    "    hyperparameters.append(\"DecisionTreeClassifier: criterion=gini splitter=best\")\n",
    "    models.append(DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\").fit(features, labels))\n",
    "    hyperparameters.append(\"DecisionTreeClassifier: criterion=gini splitter=random\")\n",
    "    models.append(DecisionTreeClassifier(criterion=\"entropy\", splitter=\"random\").fit(features, labels))\n",
    "    # For all 10 runs splitter=random was atleast slightly higher and sometimes a lot higher\n",
    "    return models, hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(features, labels):\n",
    "    models, hyperparameters = [], []\n",
    "    iterations = 3000 # had to change this many times bc some experiments needed way more \n",
    "    hyperparameters.append(\"MLPClassifier: activation=adam solver=relu\")\n",
    "    models.append(MLPClassifier(activation=\"relu\", solver=\"adam\", max_iter=iterations).fit(features, labels))\n",
    "    hyperparameters.append(\"MLPClassifier: activation=relu solver=adam hidden_layer_sizes=4,1\")\n",
    "    models.append(MLPClassifier(activation=\"relu\", solver=\"adam\", hidden_layer_sizes=(4,1), max_iter=iterations).fit(features, labels))\n",
    "    hyperparameters.append(\"MLPClassifier: activation=relu solver=adam hidden_layer_sizes=16,4\")\n",
    "    models.append(MLPClassifier(activation=\"relu\", solver=\"adam\", hidden_layer_sizes=(16,4), max_iter=iterations).fit(features, labels))\n",
    "    hyperparameters.append(\"MLPClassifier: activation=relu solver=adam hidden_layer_sizes=64,16\")\n",
    "    models.append(MLPClassifier(activation=\"relu\", solver=\"adam\", hidden_layer_sizes=(64, 16), max_iter=iterations).fit(features, labels))\n",
    "    hyperparameters.append(\"MLPClassifier: activation=relu solver=adam hidden_layer_sizes=9,6\")\n",
    "    models.append(MLPClassifier(activation=\"relu\", solver=\"adam\", hidden_layer_sizes=(9,6), max_iter=iterations).fit(features, labels))\n",
    "    hyperparameters.append(\"MLPClassifier: activation=relu solver=adam hidden_layer_sizes=27,18\")\n",
    "    models.append(MLPClassifier(activation=\"relu\", solver=\"adam\", hidden_layer_sizes=(27,18), max_iter=iterations).fit(features, labels))\n",
    "    # The scores are different everytime I run, any one of the hyperparameters might score higher \n",
    "    return models, hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientBoost(features, labels):\n",
    "    models, hyperparameters = [], []\n",
    "    hyperparameters.append(\"GradientBoostClassifier: loss=deviance\")\n",
    "    models.append(GradientBoostingClassifier(loss=\"deviance\").fit(features, labels))\n",
    "    # loss=deviance gave highest scoren from loss funcs\n",
    "    hyperparameters.append(\"GradientBoostClassifier: loss=deviance n_estimators=200\")\n",
    "    models.append(GradientBoostingClassifier(loss=\"deviance\", n_estimators=200).fit(features, labels))\n",
    "    hyperparameters.append(\"GradientBoostClassifier: loss=deviance n_estimators=300\")\n",
    "    models.append(GradientBoostingClassifier(loss=\"deviance\", n_estimators=300).fit(features, labels))\n",
    "    # n_estimators=300 gives a good train score\n",
    "    hyperparameters.append(\"GradientBoostClassifier: loss=deviance n_estimators=300 min_samples_split=4\")\n",
    "    models.append(GradientBoostingClassifier(loss=\"deviance\", n_estimators=300, min_samples_split=4).fit(features, labels))\n",
    "    hyperparameters.append(\"GradientBoostClassifier: loss=deviance n_estimators=300 min_samples_split=7\")\n",
    "    models.append(GradientBoostingClassifier(loss=\"deviance\", n_estimators=300, min_samples_split=7).fit(features, labels))\n",
    "    # min_samples_split=7 <-- this increased the test accuracy by a few percentages\n",
    "    return models, hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def experiment(features_train, labels_train, features_test, labels_test):\n",
    "    model, hyperparameter = LogisticReg(features_train, labels_train)\n",
    "    for i, j in zip(model, hyperparameter):\n",
    "        results(i, j, features_train, labels_train, features_test, labels_test)\n",
    "\n",
    "    model, hyperparameter = DecisionTree(features_train, labels_train)\n",
    "    for i, j in zip(model, hyperparameter):\n",
    "        results(i, j, features_train, labels_train, features_test, labels_test)\n",
    "\n",
    "    model, hyperparameter = MLP(features_train, labels_train)\n",
    "    for i, j in zip(model, hyperparameter):\n",
    "        results(i, j, features_train, labels_train, features_test, labels_test)\n",
    "\n",
    "    model, hyperparameter = GradientBoost(features_train, labels_train)\n",
    "    for i, j in zip(model, hyperparameter):\n",
    "        results(i, j, features_train, labels_train, features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data for experiments will be split into 10 different result boxes below this\n",
    "for i, j, k in zip(experiments_train, experiments_test, range(1, len(experiments_train)+1)):\n",
    "    print(f\"Experiment {k}:\")\n",
    "    experiment(i, y_train, j, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1:\n",
      "LogisticRegression: solver=lbfgs\n",
      "Training Accuracy: 0.8692098092643051\n",
      "Testing Accuracy: 0.8532608695652174\n",
      "Training PR_AUC: 0.9111987073889954\n",
      "Testing PR_AUC: 0.9167563998374644\n",
      "Training Precision: [0.54632153 0.87104623 1.        ]\n",
      "Testing Precision: [0.58152174 0.9        1.        ]\n",
      "Training Recall: [1.         0.89276808 0.        ]\n",
      "Testing Recall: [1.        0.8411215 0.       ]\n",
      "\n",
      "LogisticRegression: solver=newton-cg\n",
      "Training Accuracy: 0.8719346049046321\n",
      "Testing Accuracy: 0.8532608695652174\n",
      "Training PR_AUC: 0.9126423118892242\n",
      "Testing PR_AUC: 0.9167563998374644\n",
      "Training Precision: [0.54632153 0.8716707  1.        ]\n",
      "Testing Precision: [0.58152174 0.9        1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.        0.8411215 0.       ]\n",
      "\n",
      "LogisticRegression: solver=liblinear\n",
      "Training Accuracy: 0.8692098092643051\n",
      "Testing Accuracy: 0.8532608695652174\n",
      "Training PR_AUC: 0.9111987073889954\n",
      "Testing PR_AUC: 0.9167563998374644\n",
      "Training Precision: [0.54632153 0.87104623 1.        ]\n",
      "Testing Precision: [0.58152174 0.9        1.        ]\n",
      "Training Recall: [1.         0.89276808 0.        ]\n",
      "Testing Recall: [1.        0.8411215 0.       ]\n",
      "\n",
      "LogisticRegression: solver=lbfgs & penalty=l2\n",
      "Training Accuracy: 0.8719346049046321\n",
      "Testing Accuracy: 0.8532608695652174\n",
      "Training PR_AUC: 0.9126423118892242\n",
      "Testing PR_AUC: 0.9167563998374644\n",
      "Training Precision: [0.54632153 0.8716707  1.        ]\n",
      "Testing Precision: [0.58152174 0.9        1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.        0.8411215 0.       ]\n",
      "\n",
      "LogisticRegression: solver=lbfgs & penalty=none\n",
      "Training Accuracy: 0.8719346049046321\n",
      "Testing Accuracy: 0.8532608695652174\n",
      "Training PR_AUC: 0.9126423118892242\n",
      "Testing PR_AUC: 0.9167563998374644\n",
      "Training Precision: [0.54632153 0.8716707  1.        ]\n",
      "Testing Precision: [0.58152174 0.9        1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.        0.8411215 0.       ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.7989130434782609\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8872825654681111\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.87234043 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.76635514 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=entropy\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.8097826086956522\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.9045481606078073\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.91860465 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.73831776 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini splitter=best\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.7989130434782609\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8967787012275215\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.90697674 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.72897196 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini splitter=random\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.8043478260869565\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8862717996708245\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.85858586 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.79439252 0.        ]\n",
      "\n",
      "MLPClassifier: activation=adam solver=relu\n",
      "Training Accuracy: 0.8787465940054496\n",
      "Testing Accuracy: 0.8532608695652174\n",
      "Training PR_AUC: 0.9183595035176852\n",
      "Testing PR_AUC: 0.9147903371019273\n",
      "Training Precision: [0.54632153 0.88423645 1.        ]\n",
      "Testing Precision: [0.58152174 0.89215686 1.        ]\n",
      "Training Recall: [1.         0.89526185 0.        ]\n",
      "Testing Recall: [1.         0.85046729 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=4,1\n",
      "Training Accuracy: 0.8732970027247956\n",
      "Testing Accuracy: 0.875\n",
      "Training PR_AUC: 0.9147650815646218\n",
      "Testing PR_AUC: 0.9285052704543825\n",
      "Training Precision: [0.54632153 0.87931034 1.        ]\n",
      "Testing Precision: [0.58152174 0.91176471 1.        ]\n",
      "Training Recall: [1.         0.89027431 0.        ]\n",
      "Testing Recall: [1.         0.86915888 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=16,4\n",
      "Training Accuracy: 0.8719346049046321\n",
      "Testing Accuracy: 0.8641304347826086\n",
      "Training PR_AUC: 0.9113737343361292\n",
      "Testing PR_AUC: 0.9197383021285905\n",
      "Training Precision: [0.54632153 0.86460808 1.        ]\n",
      "Testing Precision: [0.58152174 0.89423077 1.        ]\n",
      "Training Recall: [1.         0.90773067 0.        ]\n",
      "Testing Recall: [1.         0.86915888 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=64,16\n",
      "Training Accuracy: 0.8732970027247956\n",
      "Testing Accuracy: 0.8641304347826086\n",
      "Training PR_AUC: 0.9107045403697337\n",
      "Testing PR_AUC: 0.9179746498202142\n",
      "Training Precision: [0.54632153 0.85648148 1.        ]\n",
      "Testing Precision: [0.58152174 0.88679245 1.        ]\n",
      "Training Recall: [1.         0.92269327 0.        ]\n",
      "Testing Recall: [1.         0.87850467 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=9,6\n",
      "Training Accuracy: 0.8719346049046321\n",
      "Testing Accuracy: 0.8478260869565217\n",
      "Training PR_AUC: 0.9136848477257672\n",
      "Testing PR_AUC: 0.9104594785449123\n",
      "Training Precision: [0.54632153 0.87714988 1.        ]\n",
      "Testing Precision: [0.58152174 0.88349515 1.        ]\n",
      "Training Recall: [1.         0.89027431 0.        ]\n",
      "Testing Recall: [1.         0.85046729 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=27,18\n",
      "Training Accuracy: 0.885558583106267\n",
      "Testing Accuracy: 0.842391304347826\n",
      "Training PR_AUC: 0.9260982794969065\n",
      "Testing PR_AUC: 0.9098008939455506\n",
      "Training Precision: [0.54632153 0.90537084 1.        ]\n",
      "Testing Precision: [0.58152174 0.89       1.        ]\n",
      "Training Recall: [1.         0.88279302 0.        ]\n",
      "Testing Recall: [1.        0.8317757 0.       ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance\n",
      "Training Accuracy: 0.946866485013624\n",
      "Testing Accuracy: 0.875\n",
      "Training PR_AUC: 0.9627627833632246\n",
      "Testing PR_AUC: 0.9285052704543825\n",
      "Training Precision: [0.54632153 0.94362745 1.        ]\n",
      "Testing Precision: [0.58152174 0.91176471 1.        ]\n",
      "Training Recall: [1.         0.96009975 0.        ]\n",
      "Testing Recall: [1.         0.86915888 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=200\n",
      "Training Accuracy: 0.9863760217983651\n",
      "Testing Accuracy: 0.8804347826086957\n",
      "Training PR_AUC: 0.9890406224061186\n",
      "Testing PR_AUC: 0.9330189571171664\n",
      "Training Precision: [0.54632153 0.98034398 1.        ]\n",
      "Testing Precision: [0.58152174 0.92079208 1.        ]\n",
      "Training Recall: [1.         0.99501247 0.        ]\n",
      "Testing Recall: [1.         0.86915888 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300\n",
      "Training Accuracy: 0.9986376021798365\n",
      "Testing Accuracy: 0.8804347826086957\n",
      "Training PR_AUC: 0.9987562189054726\n",
      "Testing PR_AUC: 0.9353138762677567\n",
      "Training Precision: [0.99751244 1.        ]\n",
      "Testing Precision: [0.58152174 0.92929293 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.85981308 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300 min_samples_split=4\n",
      "Training Accuracy: 0.9986376021798365\n",
      "Testing Accuracy: 0.8695652173913043\n",
      "Training PR_AUC: 0.9987562189054726\n",
      "Testing PR_AUC: 0.9261129561757476\n",
      "Training Precision: [0.99751244 1.        ]\n",
      "Testing Precision: [0.58152174 0.91089109 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.85981308 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300 min_samples_split=7\n",
      "Training Accuracy: 0.9986376021798365\n",
      "Testing Accuracy: 0.8804347826086957\n",
      "Training PR_AUC: 0.9987562189054726\n",
      "Testing PR_AUC: 0.9330189571171664\n",
      "Training Precision: [0.99751244 1.        ]\n",
      "Testing Precision: [0.58152174 0.92079208 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.86915888 0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e = 0\n",
    "print(f\"Experiment {e+1}:\")\n",
    "experiment(experiments_train[e], y_train, experiments_test[e], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 2:\n",
      "LogisticRegression: solver=lbfgs\n",
      "Training Accuracy: 0.8692098092643051\n",
      "Testing Accuracy: 0.8532608695652174\n",
      "Training PR_AUC: 0.9111987073889954\n",
      "Testing PR_AUC: 0.9167563998374644\n",
      "Training Precision: [0.54632153 0.87104623 1.        ]\n",
      "Testing Precision: [0.58152174 0.9        1.        ]\n",
      "Training Recall: [1.         0.89276808 0.        ]\n",
      "Testing Recall: [1.        0.8411215 0.       ]\n",
      "\n",
      "LogisticRegression: solver=newton-cg\n",
      "Training Accuracy: 0.8692098092643051\n",
      "Testing Accuracy: 0.8532608695652174\n",
      "Training PR_AUC: 0.9111987073889954\n",
      "Testing PR_AUC: 0.9167563998374644\n",
      "Training Precision: [0.54632153 0.87104623 1.        ]\n",
      "Testing Precision: [0.58152174 0.9        1.        ]\n",
      "Training Recall: [1.         0.89276808 0.        ]\n",
      "Testing Recall: [1.        0.8411215 0.       ]\n",
      "\n",
      "LogisticRegression: solver=liblinear\n",
      "Training Accuracy: 0.8692098092643051\n",
      "Testing Accuracy: 0.8532608695652174\n",
      "Training PR_AUC: 0.9111987073889954\n",
      "Testing PR_AUC: 0.9167563998374644\n",
      "Training Precision: [0.54632153 0.87104623 1.        ]\n",
      "Testing Precision: [0.58152174 0.9        1.        ]\n",
      "Training Recall: [1.         0.89276808 0.        ]\n",
      "Testing Recall: [1.        0.8411215 0.       ]\n",
      "\n",
      "LogisticRegression: solver=lbfgs & penalty=l2\n",
      "Training Accuracy: 0.8692098092643051\n",
      "Testing Accuracy: 0.8532608695652174\n",
      "Training PR_AUC: 0.9111987073889954\n",
      "Testing PR_AUC: 0.9167563998374644\n",
      "Training Precision: [0.54632153 0.87104623 1.        ]\n",
      "Testing Precision: [0.58152174 0.9        1.        ]\n",
      "Training Recall: [1.         0.89276808 0.        ]\n",
      "Testing Recall: [1.        0.8411215 0.       ]\n",
      "\n",
      "LogisticRegression: solver=lbfgs & penalty=none\n",
      "Training Accuracy: 0.8719346049046321\n",
      "Testing Accuracy: 0.8532608695652174\n",
      "Training PR_AUC: 0.9126423118892242\n",
      "Testing PR_AUC: 0.9167563998374644\n",
      "Training Precision: [0.54632153 0.8716707  1.        ]\n",
      "Testing Precision: [0.58152174 0.9        1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.        0.8411215 0.       ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.8260869565217391\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8984889524100723\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.87128713 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.82242991 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=entropy\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.8152173913043478\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8993044177338524\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.89247312 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.77570093 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini splitter=best\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.8097826086956522\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8967645266151971\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.89130435 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.76635514 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini splitter=random\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.8043478260869565\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8846769505272347\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.85148515 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.80373832 0.        ]\n",
      "\n",
      "MLPClassifier: activation=adam solver=relu\n",
      "Training Accuracy: 0.9986376021798365\n",
      "Testing Accuracy: 0.8478260869565217\n",
      "Training PR_AUC: 0.9987562189054726\n",
      "Testing PR_AUC: 0.9123009542929099\n",
      "Training Precision: [0.99751244 1.        ]\n",
      "Testing Precision: [0.58152174 0.89108911 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.        0.8411215 0.       ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=4,1\n",
      "Training Accuracy: 0.9019073569482289\n",
      "Testing Accuracy: 0.8478260869565217\n",
      "Training PR_AUC: 0.9371650170656204\n",
      "Testing PR_AUC: 0.9164536354763172\n",
      "Training Precision: [0.54632153 0.92071611 1.        ]\n",
      "Testing Precision: [0.58152174 0.90721649 1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.         0.82242991 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=16,4\n",
      "Training Accuracy: 0.9918256130790191\n",
      "Testing Accuracy: 0.8260869565217391\n",
      "Training PR_AUC: 0.9926289926289926\n",
      "Testing PR_AUC: 0.8984889524100723\n",
      "Training Precision: [0.98525799 1.        ]\n",
      "Testing Precision: [0.58152174 0.87128713 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.82242991 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=64,16\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.875\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.9285052704543825\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.91176471 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.86915888 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=9,6\n",
      "Training Accuracy: 0.9318801089918256\n",
      "Testing Accuracy: 0.8586956521739131\n",
      "Training PR_AUC: 0.9552170310737467\n",
      "Testing PR_AUC: 0.921301854382919\n",
      "Training Precision: [0.54632153 0.93984962 1.        ]\n",
      "Testing Precision: [0.58152174 0.90909091 1.        ]\n",
      "Training Recall: [1.         0.93516209 0.        ]\n",
      "Testing Recall: [1.        0.8411215 0.       ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=27,18\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.8152173913043478\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8932778106132434\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.86868687 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.80373832 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance\n",
      "Training Accuracy: 0.946866485013624\n",
      "Testing Accuracy: 0.8804347826086957\n",
      "Training PR_AUC: 0.9627627833632246\n",
      "Testing PR_AUC: 0.9308891030167703\n",
      "Training Precision: [0.54632153 0.94362745 1.        ]\n",
      "Testing Precision: [0.58152174 0.91262136 1.        ]\n",
      "Training Recall: [1.         0.96009975 0.        ]\n",
      "Testing Recall: [1.         0.87850467 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=200\n",
      "Training Accuracy: 0.9863760217983651\n",
      "Testing Accuracy: 0.8913043478260869\n",
      "Training PR_AUC: 0.9890406224061186\n",
      "Testing PR_AUC: 0.9356322923318047\n",
      "Training Precision: [0.54632153 0.98034398 1.        ]\n",
      "Testing Precision: [0.58152174 0.91428571 1.        ]\n",
      "Training Recall: [1.         0.99501247 0.        ]\n",
      "Testing Recall: [1.         0.89719626 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300\n",
      "Training Accuracy: 0.9986376021798365\n",
      "Testing Accuracy: 0.8858695652173914\n",
      "Training PR_AUC: 0.9987562189054726\n",
      "Testing PR_AUC: 0.9332646985278029\n",
      "Training Precision: [0.99751244 1.        ]\n",
      "Testing Precision: [0.58152174 0.91346154 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.88785047 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300 min_samples_split=4\n",
      "Training Accuracy: 0.9986376021798365\n",
      "Testing Accuracy: 0.875\n",
      "Training PR_AUC: 0.9987562189054726\n",
      "Testing PR_AUC: 0.9306674116212921\n",
      "Training Precision: [0.99751244 1.        ]\n",
      "Testing Precision: [0.58152174 0.92       1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.85981308 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300 min_samples_split=7\n",
      "Training Accuracy: 0.9986376021798365\n",
      "Testing Accuracy: 0.8804347826086957\n",
      "Training PR_AUC: 0.9987562189054726\n",
      "Testing PR_AUC: 0.928914881677986\n",
      "Training Precision: [0.99751244 1.        ]\n",
      "Testing Precision: [0.58152174 0.9047619  1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.88785047 0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Putting in separate cells because results is too long\n",
    "e = 1\n",
    "print(f\"Experiment {e+1}:\")\n",
    "experiment(experiments_train[e], y_train, experiments_test[e], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 3:\n",
      "LogisticRegression: solver=lbfgs\n",
      "Training Accuracy: 0.8664850136239782\n",
      "Testing Accuracy: 0.8804347826086957\n",
      "Training PR_AUC: 0.9094164784938943\n",
      "Testing PR_AUC: 0.928914881677986\n",
      "Training Precision: [0.54632153 0.86861314 1.        ]\n",
      "Testing Precision: [0.58152174 0.9047619  1.        ]\n",
      "Training Recall: [1.         0.89027431 0.        ]\n",
      "Testing Recall: [1.         0.88785047 0.        ]\n",
      "\n",
      "LogisticRegression: solver=newton-cg\n",
      "Training Accuracy: 0.8664850136239782\n",
      "Testing Accuracy: 0.8804347826086957\n",
      "Training PR_AUC: 0.9094164784938943\n",
      "Testing PR_AUC: 0.928914881677986\n",
      "Training Precision: [0.54632153 0.86861314 1.        ]\n",
      "Testing Precision: [0.58152174 0.9047619  1.        ]\n",
      "Training Recall: [1.         0.89027431 0.        ]\n",
      "Testing Recall: [1.         0.88785047 0.        ]\n",
      "\n",
      "LogisticRegression: solver=liblinear\n",
      "Training Accuracy: 0.8651226158038147\n",
      "Testing Accuracy: 0.875\n",
      "Training PR_AUC: 0.9074289703690137\n",
      "Testing PR_AUC: 0.9265015003281968\n",
      "Training Precision: [0.54632153 0.86124402 1.        ]\n",
      "Testing Precision: [0.58152174 0.90384615 1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.         0.87850467 0.        ]\n",
      "\n",
      "LogisticRegression: solver=lbfgs & penalty=l2\n",
      "Training Accuracy: 0.8664850136239782\n",
      "Testing Accuracy: 0.8804347826086957\n",
      "Training PR_AUC: 0.9094164784938943\n",
      "Testing PR_AUC: 0.928914881677986\n",
      "Training Precision: [0.54632153 0.86861314 1.        ]\n",
      "Testing Precision: [0.58152174 0.9047619  1.        ]\n",
      "Training Recall: [1.         0.89027431 0.        ]\n",
      "Testing Recall: [1.         0.88785047 0.        ]\n",
      "\n",
      "LogisticRegression: solver=lbfgs & penalty=none\n",
      "Training Accuracy: 0.8678474114441417\n",
      "Testing Accuracy: 0.8804347826086957\n",
      "Training PR_AUC: 0.9104757628093652\n",
      "Testing PR_AUC: 0.9270875660300691\n",
      "Training Precision: [0.54632153 0.87073171 1.        ]\n",
      "Testing Precision: [0.58152174 0.89719626 1.        ]\n",
      "Training Recall: [1.         0.89027431 0.        ]\n",
      "Testing Recall: [1.         0.89719626 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.8043478260869565\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8846769505272347\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.85148515 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.80373832 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=entropy\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.7989130434782609\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8872825654681111\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.87234043 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.76635514 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini splitter=best\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.8152173913043478\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8971286276438761\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.88421053 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.78504673 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini splitter=random\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.7717391304347826\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8599724509690623\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.79816514 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.81308411 0.        ]\n",
      "\n",
      "MLPClassifier: activation=adam solver=relu\n",
      "Training Accuracy: 0.9904632152588556\n",
      "Testing Accuracy: 0.8586956521739131\n",
      "Training PR_AUC: 0.9933278239730835\n",
      "Testing PR_AUC: 0.9123137458107952\n",
      "Training Precision: [0.54632153 0.99004975 1.        ]\n",
      "Testing Precision: [0.58152174 0.87155963 1.        ]\n",
      "Training Recall: [1.        0.9925187 0.       ]\n",
      "Testing Recall: [1.         0.88785047 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=4,1\n",
      "Training Accuracy: 0.8760217983651226\n",
      "Testing Accuracy: 0.8695652173913043\n",
      "Training PR_AUC: 0.9173302949196342\n",
      "Testing PR_AUC: 0.9240792281928176\n",
      "Training Precision: [0.54632153 0.88557214 1.        ]\n",
      "Testing Precision: [0.58152174 0.90291262 1.        ]\n",
      "Training Recall: [1.         0.88778055 0.        ]\n",
      "Testing Recall: [1.         0.86915888 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=16,4\n",
      "Training Accuracy: 0.9250681198910081\n",
      "Testing Accuracy: 0.8695652173913043\n",
      "Training PR_AUC: 0.9489120901187253\n",
      "Testing PR_AUC: 0.9240792281928176\n",
      "Training Precision: [0.54632153 0.92610837 1.        ]\n",
      "Testing Precision: [0.58152174 0.90291262 1.        ]\n",
      "Training Recall: [1.         0.93765586 0.        ]\n",
      "Testing Recall: [1.         0.86915888 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=64,16\n",
      "Training Accuracy: 0.9986376021798365\n",
      "Testing Accuracy: 0.8206521739130435\n",
      "Training PR_AUC: 0.9987562189054726\n",
      "Testing PR_AUC: 0.8900082020256745\n",
      "Training Precision: [0.99751244 1.        ]\n",
      "Testing Precision: [0.58152174 0.84259259 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.85046729 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=9,6\n",
      "Training Accuracy: 0.8869209809264306\n",
      "Testing Accuracy: 0.8260869565217391\n",
      "Training PR_AUC: 0.9233569536249135\n",
      "Testing PR_AUC: 0.8984889524100723\n",
      "Training Precision: [0.54632153 0.88970588 1.        ]\n",
      "Testing Precision: [0.58152174 0.87128713 1.        ]\n",
      "Training Recall: [1.         0.90523691 0.        ]\n",
      "Testing Recall: [1.         0.82242991 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=27,18\n",
      "Training Accuracy: 0.9945504087193461\n",
      "Testing Accuracy: 0.8369565217391305\n",
      "Training PR_AUC: 0.9963748666480937\n",
      "Testing PR_AUC: 0.899228422100362\n",
      "Training Precision: [0.54632153 0.99501247 1.        ]\n",
      "Testing Precision: [0.58152174 0.85321101 1.        ]\n",
      "Training Recall: [1.         0.99501247 0.        ]\n",
      "Testing Recall: [1.         0.86915888 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance\n",
      "Training Accuracy: 0.9427792915531336\n",
      "Testing Accuracy: 0.8804347826086957\n",
      "Training PR_AUC: 0.9608569444958265\n",
      "Testing PR_AUC: 0.928914881677986\n",
      "Training Precision: [0.54632153 0.94320988 1.        ]\n",
      "Testing Precision: [0.58152174 0.9047619  1.        ]\n",
      "Training Recall: [1.         0.95261845 0.        ]\n",
      "Testing Recall: [1.         0.88785047 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=200\n",
      "Training Accuracy: 0.9836512261580381\n",
      "Testing Accuracy: 0.8858695652173914\n",
      "Training PR_AUC: 0.986643693358823\n",
      "Testing PR_AUC: 0.9295190151549355\n",
      "Training Precision: [0.54632153 0.97555012 1.        ]\n",
      "Testing Precision: [0.58152174 0.89814815 1.        ]\n",
      "Training Recall: [1.         0.99501247 0.        ]\n",
      "Testing Recall: [1.         0.90654206 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300\n",
      "Training Accuracy: 0.9986376021798365\n",
      "Testing Accuracy: 0.8913043478260869\n",
      "Training PR_AUC: 0.9987562189054726\n",
      "Testing PR_AUC: 0.9303018988106351\n",
      "Training Precision: [0.99751244 1.        ]\n",
      "Testing Precision: [0.58152174 0.89189189 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.92523364 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300 min_samples_split=4\n",
      "Training Accuracy: 0.9986376021798365\n",
      "Testing Accuracy: 0.8586956521739131\n",
      "Training PR_AUC: 0.9987562189054726\n",
      "Testing PR_AUC: 0.913830759853718\n",
      "Training Precision: [0.99751244 1.        ]\n",
      "Testing Precision: [0.58152174 0.87850467 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.87850467 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300 min_samples_split=7\n",
      "Training Accuracy: 0.9986376021798365\n",
      "Testing Accuracy: 0.8804347826086957\n",
      "Training PR_AUC: 0.9987562189054726\n",
      "Testing PR_AUC: 0.9253990695212284\n",
      "Training Precision: [0.99751244 1.        ]\n",
      "Testing Precision: [0.58152174 0.88990826 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.90654206 0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Putting in separate cells because results is too long\n",
    "e = 2\n",
    "print(f\"Experiment {e+1}:\")\n",
    "experiment(experiments_train[e], y_train, experiments_test[e], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 4:\n",
      "LogisticRegression: solver=lbfgs\n",
      "Training Accuracy: 0.8692098092643051\n",
      "Testing Accuracy: 0.842391304347826\n",
      "Training PR_AUC: 0.910541900558672\n",
      "Testing PR_AUC: 0.9118249798903715\n",
      "Training Precision: [0.54632153 0.86746988 1.        ]\n",
      "Testing Precision: [0.58152174 0.89795918 1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.         0.82242991 0.        ]\n",
      "\n",
      "LogisticRegression: solver=newton-cg\n",
      "Training Accuracy: 0.8692098092643051\n",
      "Testing Accuracy: 0.842391304347826\n",
      "Training PR_AUC: 0.910541900558672\n",
      "Testing PR_AUC: 0.9118249798903715\n",
      "Training Precision: [0.54632153 0.86746988 1.        ]\n",
      "Testing Precision: [0.58152174 0.89795918 1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.         0.82242991 0.        ]\n",
      "\n",
      "LogisticRegression: solver=liblinear\n",
      "Training Accuracy: 0.8692098092643051\n",
      "Testing Accuracy: 0.8532608695652174\n",
      "Training PR_AUC: 0.910541900558672\n",
      "Testing PR_AUC: 0.9167563998374644\n",
      "Training Precision: [0.54632153 0.86746988 1.        ]\n",
      "Testing Precision: [0.58152174 0.9        1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.        0.8411215 0.       ]\n",
      "\n",
      "LogisticRegression: solver=lbfgs & penalty=l2\n",
      "Training Accuracy: 0.8692098092643051\n",
      "Testing Accuracy: 0.842391304347826\n",
      "Training PR_AUC: 0.910541900558672\n",
      "Testing PR_AUC: 0.9118249798903715\n",
      "Training Precision: [0.54632153 0.86746988 1.        ]\n",
      "Testing Precision: [0.58152174 0.89795918 1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.         0.82242991 0.        ]\n",
      "\n",
      "LogisticRegression: solver=lbfgs & penalty=none\n",
      "Training Accuracy: 0.8719346049046321\n",
      "Testing Accuracy: 0.8369565217391305\n",
      "Training PR_AUC: 0.9126423118892242\n",
      "Testing PR_AUC: 0.9115659552171775\n",
      "Training Precision: [0.54632153 0.8716707  1.        ]\n",
      "Testing Precision: [0.58152174 0.90526316 1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.         0.80373832 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.7010869565217391\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8303163496892993\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.81707317 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.62616822 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=entropy\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.7717391304347826\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8699770178221109\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.84946237 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.73831776 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini splitter=best\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.7554347826086957\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8567041683597454\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.82291667 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.73831776 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini splitter=random\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.8152173913043478\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8951232002748024\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.87628866 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.79439252 0.        ]\n",
      "\n",
      "MLPClassifier: activation=adam solver=relu\n",
      "Training Accuracy: 0.9495912806539509\n",
      "Testing Accuracy: 0.875\n",
      "Training PR_AUC: 0.9630185566897956\n",
      "Testing PR_AUC: 0.9329976200150926\n",
      "Training Precision: [0.54632153 0.93961353 1.        ]\n",
      "Testing Precision: [0.58152174 0.92857143 1.        ]\n",
      "Training Recall: [1.         0.97007481 0.        ]\n",
      "Testing Recall: [1.         0.85046729 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=4,1\n",
      "Training Accuracy: 0.8692098092643051\n",
      "Testing Accuracy: 0.8532608695652174\n",
      "Training PR_AUC: 0.9108659742532618\n",
      "Testing PR_AUC: 0.9188825265986118\n",
      "Training Precision: [0.54632153 0.86924939 1.        ]\n",
      "Testing Precision: [0.58152174 0.90816327 1.        ]\n",
      "Training Recall: [1.         0.89526185 0.        ]\n",
      "Testing Recall: [1.        0.8317757 0.       ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=16,4\n",
      "Training Accuracy: 0.9291553133514986\n",
      "Testing Accuracy: 0.8532608695652174\n",
      "Training PR_AUC: 0.9508801379494974\n",
      "Testing PR_AUC: 0.9236558396085314\n",
      "Training Precision: [0.54632153 0.92665037 1.        ]\n",
      "Testing Precision: [0.58152174 0.92553191 1.        ]\n",
      "Training Recall: [1.         0.94513716 0.        ]\n",
      "Testing Recall: [1.         0.81308411 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=64,16\n",
      "Training Accuracy: 0.9550408719346049\n",
      "Testing Accuracy: 0.8315217391304348\n",
      "Training PR_AUC: 0.9665653833928651\n",
      "Testing PR_AUC: 0.9028453880536367\n",
      "Training Precision: [0.54632153 0.94444444 1.        ]\n",
      "Testing Precision: [0.58152174 0.88       1.        ]\n",
      "Training Recall: [1.         0.97506234 0.        ]\n",
      "Testing Recall: [1.         0.82242991 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=9,6\n",
      "Training Accuracy: 0.8923705722070845\n",
      "Testing Accuracy: 0.8532608695652174\n",
      "Training PR_AUC: 0.9250475316864091\n",
      "Testing PR_AUC: 0.9129751039289845\n",
      "Training Precision: [0.54632153 0.88516746 1.        ]\n",
      "Testing Precision: [0.58152174 0.88461538 1.        ]\n",
      "Training Recall: [1.         0.92269327 0.        ]\n",
      "Testing Recall: [1.         0.85981308 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=27,18\n",
      "Training Accuracy: 0.9509536784741145\n",
      "Testing Accuracy: 0.8043478260869565\n",
      "Training PR_AUC: 0.9641561033321322\n",
      "Testing PR_AUC: 0.8942117918651848\n",
      "Training Precision: [0.54632153 0.94188862 1.        ]\n",
      "Testing Precision: [0.58152174 0.89010989 1.        ]\n",
      "Training Recall: [1.         0.97007481 0.        ]\n",
      "Testing Recall: [1.         0.75700935 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance\n",
      "Training Accuracy: 0.946866485013624\n",
      "Testing Accuracy: 0.8586956521739131\n",
      "Training PR_AUC: 0.9627627833632246\n",
      "Testing PR_AUC: 0.9314618487972814\n",
      "Training Precision: [0.54632153 0.94362745 1.        ]\n",
      "Testing Precision: [0.58152174 0.94505495 1.        ]\n",
      "Training Recall: [1.         0.96009975 0.        ]\n",
      "Testing Recall: [1.         0.80373832 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=200\n",
      "Training Accuracy: 0.9863760217983651\n",
      "Testing Accuracy: 0.8967391304347826\n",
      "Training PR_AUC: 0.9890406224061186\n",
      "Testing PR_AUC: 0.9422202038068377\n",
      "Training Precision: [0.54632153 0.98034398 1.        ]\n",
      "Testing Precision: [0.58152174 0.93137255 1.        ]\n",
      "Training Recall: [1.         0.99501247 0.        ]\n",
      "Testing Recall: [1.         0.88785047 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300\n",
      "Training Accuracy: 0.9986376021798365\n",
      "Testing Accuracy: 0.875\n",
      "Training PR_AUC: 0.9987562189054726\n",
      "Testing PR_AUC: 0.9285052704543825\n",
      "Training Precision: [0.99751244 1.        ]\n",
      "Testing Precision: [0.58152174 0.91176471 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.86915888 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300 min_samples_split=4\n",
      "Training Accuracy: 0.9986376021798365\n",
      "Testing Accuracy: 0.875\n",
      "Training PR_AUC: 0.9987562189054726\n",
      "Testing PR_AUC: 0.9306674116212921\n",
      "Training Precision: [0.99751244 1.        ]\n",
      "Testing Precision: [0.58152174 0.92       1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.85981308 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300 min_samples_split=7\n",
      "Training Accuracy: 0.9986376021798365\n",
      "Testing Accuracy: 0.8532608695652174\n",
      "Training PR_AUC: 0.9987562189054726\n",
      "Testing PR_AUC: 0.9188825265986118\n",
      "Training Precision: [0.99751244 1.        ]\n",
      "Testing Precision: [0.58152174 0.90816327 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.        0.8317757 0.       ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Putting in separate cells because results is too long\n",
    "e = 3\n",
    "print(f\"Experiment {e+1}:\")\n",
    "experiment(experiments_train[e], y_train, experiments_test[e], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 5:\n",
      "LogisticRegression: solver=lbfgs\n",
      "Training Accuracy: 0.8705722070844687\n",
      "Testing Accuracy: 0.8586956521739131\n",
      "Training PR_AUC: 0.9119208885671855\n",
      "Testing PR_AUC: 0.9192069552343288\n",
      "Training Precision: [0.54632153 0.87135922 1.        ]\n",
      "Testing Precision: [0.58152174 0.9009901  1.        ]\n",
      "Training Recall: [1.         0.89526185 0.        ]\n",
      "Testing Recall: [1.         0.85046729 0.        ]\n",
      "\n",
      "LogisticRegression: solver=newton-cg\n",
      "Training Accuracy: 0.8692098092643051\n",
      "Testing Accuracy: 0.8532608695652174\n",
      "Training PR_AUC: 0.9111987073889954\n",
      "Testing PR_AUC: 0.9167563998374644\n",
      "Training Precision: [0.54632153 0.87104623 1.        ]\n",
      "Testing Precision: [0.58152174 0.9        1.        ]\n",
      "Training Recall: [1.         0.89276808 0.        ]\n",
      "Testing Recall: [1.        0.8411215 0.       ]\n",
      "\n",
      "LogisticRegression: solver=liblinear\n",
      "Training Accuracy: 0.8692098092643051\n",
      "Testing Accuracy: 0.8532608695652174\n",
      "Training PR_AUC: 0.9111987073889954\n",
      "Testing PR_AUC: 0.9167563998374644\n",
      "Training Precision: [0.54632153 0.87104623 1.        ]\n",
      "Testing Precision: [0.58152174 0.9        1.        ]\n",
      "Training Recall: [1.         0.89276808 0.        ]\n",
      "Testing Recall: [1.        0.8411215 0.       ]\n",
      "\n",
      "LogisticRegression: solver=lbfgs & penalty=l2\n",
      "Training Accuracy: 0.8692098092643051\n",
      "Testing Accuracy: 0.8532608695652174\n",
      "Training PR_AUC: 0.9111987073889954\n",
      "Testing PR_AUC: 0.9167563998374644\n",
      "Training Precision: [0.54632153 0.87104623 1.        ]\n",
      "Testing Precision: [0.58152174 0.9        1.        ]\n",
      "Training Recall: [1.         0.89276808 0.        ]\n",
      "Testing Recall: [1.        0.8411215 0.       ]\n",
      "\n",
      "LogisticRegression: solver=lbfgs & penalty=none\n",
      "Training Accuracy: 0.8719346049046321\n",
      "Testing Accuracy: 0.8532608695652174\n",
      "Training PR_AUC: 0.9126423118892242\n",
      "Testing PR_AUC: 0.9167563998374644\n",
      "Training Precision: [0.54632153 0.8716707  1.        ]\n",
      "Testing Precision: [0.58152174 0.9        1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.        0.8411215 0.       ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.8152173913043478\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8932778106132434\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.86868687 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.80373832 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=entropy\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.7934782608695652\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8972106426518753\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.91566265 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.71028037 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini splitter=best\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.7880434782608695\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8800079106400269\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.86170213 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.75700935 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini splitter=random\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.8206521739130435\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8996872037112285\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.88541667 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.79439252 0.        ]\n",
      "\n",
      "MLPClassifier: activation=adam solver=relu\n",
      "Training Accuracy: 0.8664850136239782\n",
      "Testing Accuracy: 0.8532608695652174\n",
      "Training PR_AUC: 0.9051540792687708\n",
      "Testing PR_AUC: 0.9070608637603761\n",
      "Training Precision: [0.54632153 0.84198646 1.        ]\n",
      "Testing Precision: [0.58152174 0.85714286 1.        ]\n",
      "Training Recall: [1.         0.93017456 0.        ]\n",
      "Testing Recall: [1.         0.89719626 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=4,1\n",
      "Training Accuracy: 0.5463215258855586\n",
      "Testing Accuracy: 0.5815217391304348\n",
      "Training PR_AUC: 0.7731607629427792\n",
      "Testing PR_AUC: 0.7907608695652174\n",
      "Training Precision: [0.54632153 1.        ]\n",
      "Testing Precision: [0.58152174 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1. 0.]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=16,4\n",
      "Training Accuracy: 0.5463215258855586\n",
      "Testing Accuracy: 0.5815217391304348\n",
      "Training PR_AUC: 0.7731607629427792\n",
      "Testing PR_AUC: 0.7907608695652174\n",
      "Training Precision: [0.54632153 1.        ]\n",
      "Testing Precision: [0.58152174 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1. 0.]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=64,16\n",
      "Training Accuracy: 0.8896457765667575\n",
      "Testing Accuracy: 0.875\n",
      "Training PR_AUC: 0.9216424230112031\n",
      "Testing PR_AUC: 0.9246471368442036\n",
      "Training Precision: [0.54632153 0.87383178 1.        ]\n",
      "Testing Precision: [0.58152174 0.89622642 1.        ]\n",
      "Training Recall: [1.         0.93266833 0.        ]\n",
      "Testing Recall: [1.         0.88785047 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=9,6\n",
      "Training Accuracy: 0.8732970027247956\n",
      "Testing Accuracy: 0.8695652173913043\n",
      "Training PR_AUC: 0.9144010832298428\n",
      "Testing PR_AUC: 0.9261129561757476\n",
      "Training Precision: [0.54632153 0.87745098 1.        ]\n",
      "Testing Precision: [0.58152174 0.91089109 1.        ]\n",
      "Training Recall: [1.         0.89276808 0.        ]\n",
      "Testing Recall: [1.         0.85981308 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=27,18\n",
      "Training Accuracy: 0.8801089918256131\n",
      "Testing Accuracy: 0.8586956521739131\n",
      "Training PR_AUC: 0.9172852603102376\n",
      "Testing PR_AUC: 0.913830759853718\n",
      "Training Precision: [0.54632153 0.87529976 1.        ]\n",
      "Testing Precision: [0.58152174 0.87850467 1.        ]\n",
      "Training Recall: [1.         0.91022444 0.        ]\n",
      "Testing Recall: [1.         0.87850467 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance\n",
      "Training Accuracy: 0.9427792915531336\n",
      "Testing Accuracy: 0.8967391304347826\n",
      "Training PR_AUC: 0.9603336606231276\n",
      "Testing PR_AUC: 0.9422202038068377\n",
      "Training Precision: [0.54632153 0.94103194 1.        ]\n",
      "Testing Precision: [0.58152174 0.93137255 1.        ]\n",
      "Training Recall: [1.         0.95511222 0.        ]\n",
      "Testing Recall: [1.         0.88785047 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=200\n",
      "Training Accuracy: 0.9850136239782016\n",
      "Testing Accuracy: 0.907608695652174\n",
      "Training PR_AUC: 0.9884507316024244\n",
      "Testing PR_AUC: 0.9446645979161715\n",
      "Training Precision: [0.54632153 0.98029557 1.        ]\n",
      "Testing Precision: [0.58152174 0.9245283  1.        ]\n",
      "Training Recall: [1.        0.9925187 0.       ]\n",
      "Testing Recall: [1.         0.91588785 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300\n",
      "Training Accuracy: 0.9986376021798365\n",
      "Testing Accuracy: 0.9021739130434783\n",
      "Training PR_AUC: 0.9987562189054726\n",
      "Testing PR_AUC: 0.9423497029856234\n",
      "Training Precision: [0.99751244 1.        ]\n",
      "Testing Precision: [0.58152174 0.92380952 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.90654206 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300 min_samples_split=4\n",
      "Training Accuracy: 0.997275204359673\n",
      "Testing Accuracy: 0.907608695652174\n",
      "Training PR_AUC: 0.9975186104218362\n",
      "Testing PR_AUC: 0.9490776704830652\n",
      "Training Precision: [0.99503722 1.        ]\n",
      "Testing Precision: [0.58152174 0.94117647 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.89719626 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300 min_samples_split=7\n",
      "Training Accuracy: 0.9986376021798365\n",
      "Testing Accuracy: 0.9021739130434783\n",
      "Training PR_AUC: 0.9987562189054726\n",
      "Testing PR_AUC: 0.946830959000004\n",
      "Training Precision: [0.99751244 1.        ]\n",
      "Testing Precision: [0.58152174 0.94059406 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.88785047 0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Putting in separate cells because results is too long\n",
    "e = 4\n",
    "print(f\"Experiment {e+1}:\")\n",
    "experiment(experiments_train[e], y_train, experiments_test[e], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 6:\n",
      "LogisticRegression: solver=lbfgs\n",
      "Training Accuracy: 0.7452316076294278\n",
      "Testing Accuracy: 0.6902173913043478\n",
      "Training PR_AUC: 0.8310425713090349\n",
      "Testing PR_AUC: 0.8119283369585136\n",
      "Training Precision: [0.54632153 0.77295918 1.        ]\n",
      "Testing Precision: [0.58152174 0.74509804 1.        ]\n",
      "Training Recall: [1.         0.75561097 0.        ]\n",
      "Testing Recall: [1.         0.71028037 0.        ]\n",
      "\n",
      "LogisticRegression: solver=newton-cg\n",
      "Training Accuracy: 0.7452316076294278\n",
      "Testing Accuracy: 0.6902173913043478\n",
      "Training PR_AUC: 0.8310425713090349\n",
      "Testing PR_AUC: 0.8119283369585136\n",
      "Training Precision: [0.54632153 0.77295918 1.        ]\n",
      "Testing Precision: [0.58152174 0.74509804 1.        ]\n",
      "Training Recall: [1.         0.75561097 0.        ]\n",
      "Testing Recall: [1.         0.71028037 0.        ]\n",
      "\n",
      "LogisticRegression: solver=liblinear\n",
      "Training Accuracy: 0.7452316076294278\n",
      "Testing Accuracy: 0.6902173913043478\n",
      "Training PR_AUC: 0.8310425713090349\n",
      "Testing PR_AUC: 0.8119283369585136\n",
      "Training Precision: [0.54632153 0.77295918 1.        ]\n",
      "Testing Precision: [0.58152174 0.74509804 1.        ]\n",
      "Training Recall: [1.         0.75561097 0.        ]\n",
      "Testing Recall: [1.         0.71028037 0.        ]\n",
      "\n",
      "LogisticRegression: solver=lbfgs & penalty=l2\n",
      "Training Accuracy: 0.7452316076294278\n",
      "Testing Accuracy: 0.6902173913043478\n",
      "Training PR_AUC: 0.8310425713090349\n",
      "Testing PR_AUC: 0.8119283369585136\n",
      "Training Precision: [0.54632153 0.77295918 1.        ]\n",
      "Testing Precision: [0.58152174 0.74509804 1.        ]\n",
      "Training Recall: [1.         0.75561097 0.        ]\n",
      "Testing Recall: [1.         0.71028037 0.        ]\n",
      "\n",
      "LogisticRegression: solver=lbfgs & penalty=none\n",
      "Training Accuracy: 0.7452316076294278\n",
      "Testing Accuracy: 0.6902173913043478\n",
      "Training PR_AUC: 0.8310425713090349\n",
      "Testing PR_AUC: 0.8119283369585136\n",
      "Training Precision: [0.54632153 0.77295918 1.        ]\n",
      "Testing Precision: [0.58152174 0.74509804 1.        ]\n",
      "Training Recall: [1.         0.75561097 0.        ]\n",
      "Testing Recall: [1.         0.71028037 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini\n",
      "Training Accuracy: 0.7547683923705722\n",
      "Testing Accuracy: 0.7010869565217391\n",
      "Training PR_AUC: 0.83675090861227\n",
      "Testing PR_AUC: 0.8187858036347411\n",
      "Training Precision: [0.54632153 0.77419355 1.        ]\n",
      "Testing Precision: [0.58152174 0.75490196 1.        ]\n",
      "Training Recall: [1.         0.77805486 0.        ]\n",
      "Testing Recall: [1.         0.71962617 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=entropy\n",
      "Training Accuracy: 0.7547683923705722\n",
      "Testing Accuracy: 0.7010869565217391\n",
      "Training PR_AUC: 0.83675090861227\n",
      "Testing PR_AUC: 0.8187858036347411\n",
      "Training Precision: [0.54632153 0.77419355 1.        ]\n",
      "Testing Precision: [0.58152174 0.75490196 1.        ]\n",
      "Training Recall: [1.         0.77805486 0.        ]\n",
      "Testing Recall: [1.         0.71962617 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini splitter=best\n",
      "Training Accuracy: 0.7547683923705722\n",
      "Testing Accuracy: 0.7010869565217391\n",
      "Training PR_AUC: 0.83675090861227\n",
      "Testing PR_AUC: 0.8187858036347411\n",
      "Training Precision: [0.54632153 0.77419355 1.        ]\n",
      "Testing Precision: [0.58152174 0.75490196 1.        ]\n",
      "Training Recall: [1.         0.77805486 0.        ]\n",
      "Testing Recall: [1.         0.71962617 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini splitter=random\n",
      "Training Accuracy: 0.7547683923705722\n",
      "Testing Accuracy: 0.7010869565217391\n",
      "Training PR_AUC: 0.83675090861227\n",
      "Testing PR_AUC: 0.8187858036347411\n",
      "Training Precision: [0.54632153 0.77419355 1.        ]\n",
      "Testing Precision: [0.58152174 0.75490196 1.        ]\n",
      "Training Recall: [1.         0.77805486 0.        ]\n",
      "Testing Recall: [1.         0.71962617 0.        ]\n",
      "\n",
      "MLPClassifier: activation=adam solver=relu\n",
      "Training Accuracy: 0.7534059945504087\n",
      "Testing Accuracy: 0.7065217391304348\n",
      "Training PR_AUC: 0.8353327953792938\n",
      "Testing PR_AUC: 0.8210331166192605\n",
      "Training Precision: [0.54632153 0.76570048 1.        ]\n",
      "Testing Precision: [0.58152174 0.74766355 1.        ]\n",
      "Training Recall: [1.         0.79052369 0.        ]\n",
      "Testing Recall: [1.         0.74766355 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=4,1\n",
      "Training Accuracy: 0.7479564032697548\n",
      "Testing Accuracy: 0.7119565217391305\n",
      "Training PR_AUC: 0.8327501848016234\n",
      "Testing PR_AUC: 0.8250535273341043\n",
      "Training Precision: [0.54632153 0.77411168 1.        ]\n",
      "Testing Precision: [0.58152174 0.75961538 1.        ]\n",
      "Training Recall: [1.        0.7605985 0.       ]\n",
      "Testing Recall: [1.         0.73831776 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=16,4\n",
      "Training Accuracy: 0.7520435967302452\n",
      "Testing Accuracy: 0.7010869565217391\n",
      "Training PR_AUC: 0.8371688761746859\n",
      "Testing PR_AUC: 0.8193793173506705\n",
      "Training Precision: [0.54632153 0.792      1.        ]\n",
      "Testing Precision: [0.58152174 0.76       1.        ]\n",
      "Training Recall: [1.         0.74064838 0.        ]\n",
      "Testing Recall: [1.         0.71028037 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=64,16\n",
      "Training Accuracy: 0.7534059945504087\n",
      "Testing Accuracy: 0.7065217391304348\n",
      "Training PR_AUC: 0.8353327953792938\n",
      "Testing PR_AUC: 0.8210331166192605\n",
      "Training Precision: [0.54632153 0.76570048 1.        ]\n",
      "Testing Precision: [0.58152174 0.74766355 1.        ]\n",
      "Training Recall: [1.         0.79052369 0.        ]\n",
      "Testing Recall: [1.         0.74766355 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=9,6\n",
      "Training Accuracy: 0.7506811989100818\n",
      "Testing Accuracy: 0.6902173913043478\n",
      "Training PR_AUC: 0.8339894406347957\n",
      "Testing PR_AUC: 0.811527130934892\n",
      "Training Precision: [0.54632153 0.76980198 1.        ]\n",
      "Testing Precision: [0.58152174 0.74038462 1.        ]\n",
      "Training Recall: [1.        0.7755611 0.       ]\n",
      "Testing Recall: [1.         0.71962617 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=27,18\n",
      "Training Accuracy: 0.7534059945504087\n",
      "Testing Accuracy: 0.7065217391304348\n",
      "Training PR_AUC: 0.8353327953792938\n",
      "Testing PR_AUC: 0.8210331166192605\n",
      "Training Precision: [0.54632153 0.76570048 1.        ]\n",
      "Testing Precision: [0.58152174 0.74766355 1.        ]\n",
      "Training Recall: [1.         0.79052369 0.        ]\n",
      "Testing Recall: [1.         0.74766355 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance\n",
      "Training Accuracy: 0.7520435967302452\n",
      "Testing Accuracy: 0.7010869565217391\n",
      "Training PR_AUC: 0.8350564324882617\n",
      "Testing PR_AUC: 0.8187858036347411\n",
      "Training Precision: [0.54632153 0.77306733 1.        ]\n",
      "Testing Precision: [0.58152174 0.75490196 1.        ]\n",
      "Training Recall: [1.         0.77306733 0.        ]\n",
      "Testing Recall: [1.         0.71962617 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=200\n",
      "Training Accuracy: 0.7534059945504087\n",
      "Testing Accuracy: 0.7065217391304348\n",
      "Training PR_AUC: 0.8353327953792938\n",
      "Testing PR_AUC: 0.8210331166192605\n",
      "Training Precision: [0.54632153 0.76570048 1.        ]\n",
      "Testing Precision: [0.58152174 0.74766355 1.        ]\n",
      "Training Recall: [1.         0.79052369 0.        ]\n",
      "Testing Recall: [1.         0.74766355 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300\n",
      "Training Accuracy: 0.7547683923705722\n",
      "Testing Accuracy: 0.7010869565217391\n",
      "Training PR_AUC: 0.836639571388077\n",
      "Testing PR_AUC: 0.8187858036347411\n",
      "Training Precision: [0.54632153 0.77283951 1.        ]\n",
      "Testing Precision: [0.58152174 0.75490196 1.        ]\n",
      "Training Recall: [1.         0.78054863 0.        ]\n",
      "Testing Recall: [1.         0.71962617 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300 min_samples_split=4\n",
      "Training Accuracy: 0.7547683923705722\n",
      "Testing Accuracy: 0.7010869565217391\n",
      "Training PR_AUC: 0.836639571388077\n",
      "Testing PR_AUC: 0.8187858036347411\n",
      "Training Precision: [0.54632153 0.77283951 1.        ]\n",
      "Testing Precision: [0.58152174 0.75490196 1.        ]\n",
      "Training Recall: [1.         0.78054863 0.        ]\n",
      "Testing Recall: [1.         0.71962617 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300 min_samples_split=7\n",
      "Training Accuracy: 0.7547683923705722\n",
      "Testing Accuracy: 0.7010869565217391\n",
      "Training PR_AUC: 0.836639571388077\n",
      "Testing PR_AUC: 0.8187858036347411\n",
      "Training Precision: [0.54632153 0.77283951 1.        ]\n",
      "Testing Precision: [0.58152174 0.75490196 1.        ]\n",
      "Training Recall: [1.         0.78054863 0.        ]\n",
      "Testing Recall: [1.         0.71962617 0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Putting in separate cells because results is too long\n",
    "e = 5\n",
    "print(f\"Experiment {e+1}:\")\n",
    "experiment(experiments_train[e], y_train, experiments_test[e], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 7:\n",
      "LogisticRegression: solver=lbfgs\n",
      "Training Accuracy: 0.8623978201634878\n",
      "Testing Accuracy: 0.8260869565217391\n",
      "Training PR_AUC: 0.9048251210183373\n",
      "Testing PR_AUC: 0.9043472914305268\n",
      "Training Precision: [0.54632153 0.85377358 1.        ]\n",
      "Testing Precision: [0.58152174 0.89473684 1.        ]\n",
      "Training Recall: [1.         0.90274314 0.        ]\n",
      "Testing Recall: [1.         0.79439252 0.        ]\n",
      "\n",
      "LogisticRegression: solver=newton-cg\n",
      "Training Accuracy: 0.8623978201634878\n",
      "Testing Accuracy: 0.8260869565217391\n",
      "Training PR_AUC: 0.9048251210183373\n",
      "Testing PR_AUC: 0.9043472914305268\n",
      "Training Precision: [0.54632153 0.85377358 1.        ]\n",
      "Testing Precision: [0.58152174 0.89473684 1.        ]\n",
      "Training Recall: [1.         0.90274314 0.        ]\n",
      "Testing Recall: [1.         0.79439252 0.        ]\n",
      "\n",
      "LogisticRegression: solver=liblinear\n",
      "Training Accuracy: 0.8623978201634878\n",
      "Testing Accuracy: 0.8260869565217391\n",
      "Training PR_AUC: 0.9048251210183373\n",
      "Testing PR_AUC: 0.9043472914305268\n",
      "Training Precision: [0.54632153 0.85377358 1.        ]\n",
      "Testing Precision: [0.58152174 0.89473684 1.        ]\n",
      "Training Recall: [1.         0.90274314 0.        ]\n",
      "Testing Recall: [1.         0.79439252 0.        ]\n",
      "\n",
      "LogisticRegression: solver=lbfgs & penalty=l2\n",
      "Training Accuracy: 0.8623978201634878\n",
      "Testing Accuracy: 0.8260869565217391\n",
      "Training PR_AUC: 0.9048251210183373\n",
      "Testing PR_AUC: 0.9043472914305268\n",
      "Training Precision: [0.54632153 0.85377358 1.        ]\n",
      "Testing Precision: [0.58152174 0.89473684 1.        ]\n",
      "Training Recall: [1.         0.90274314 0.        ]\n",
      "Testing Recall: [1.         0.79439252 0.        ]\n",
      "\n",
      "LogisticRegression: solver=lbfgs & penalty=none\n",
      "Training Accuracy: 0.8623978201634878\n",
      "Testing Accuracy: 0.8260869565217391\n",
      "Training PR_AUC: 0.9048251210183373\n",
      "Testing PR_AUC: 0.9043472914305268\n",
      "Training Precision: [0.54632153 0.85377358 1.        ]\n",
      "Testing Precision: [0.58152174 0.89473684 1.        ]\n",
      "Training Recall: [1.         0.90274314 0.        ]\n",
      "Testing Recall: [1.         0.79439252 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini\n",
      "Training Accuracy: 0.8746594005449592\n",
      "Testing Accuracy: 0.8152173913043478\n",
      "Training PR_AUC: 0.9147631651791979\n",
      "Testing PR_AUC: 0.9016618032516042\n",
      "Training Precision: [0.54632153 0.87591241 1.        ]\n",
      "Testing Precision: [0.58152174 0.9010989  1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.         0.76635514 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=entropy\n",
      "Training Accuracy: 0.8746594005449592\n",
      "Testing Accuracy: 0.8152173913043478\n",
      "Training PR_AUC: 0.9147631651791979\n",
      "Testing PR_AUC: 0.9016618032516042\n",
      "Training Precision: [0.54632153 0.87591241 1.        ]\n",
      "Testing Precision: [0.58152174 0.9010989  1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.         0.76635514 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini splitter=best\n",
      "Training Accuracy: 0.8746594005449592\n",
      "Testing Accuracy: 0.8152173913043478\n",
      "Training PR_AUC: 0.9147631651791979\n",
      "Testing PR_AUC: 0.9016618032516042\n",
      "Training Precision: [0.54632153 0.87591241 1.        ]\n",
      "Testing Precision: [0.58152174 0.9010989  1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.         0.76635514 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini splitter=random\n",
      "Training Accuracy: 0.8746594005449592\n",
      "Testing Accuracy: 0.8206521739130435\n",
      "Training PR_AUC: 0.9147631651791979\n",
      "Testing PR_AUC: 0.9041548151158065\n",
      "Training Precision: [0.54632153 0.87591241 1.        ]\n",
      "Testing Precision: [0.58152174 0.90217391 1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.         0.77570093 0.        ]\n",
      "\n",
      "MLPClassifier: activation=adam solver=relu\n",
      "Training Accuracy: 0.8732970027247956\n",
      "Testing Accuracy: 0.8152173913043478\n",
      "Training PR_AUC: 0.9140461549654806\n",
      "Testing PR_AUC: 0.9042130265855206\n",
      "Training Precision: [0.54632153 0.87560976 1.        ]\n",
      "Testing Precision: [0.58152174 0.91011236 1.        ]\n",
      "Training Recall: [1.         0.89526185 0.        ]\n",
      "Testing Recall: [1.         0.75700935 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=4,1\n",
      "Training Accuracy: 0.8651226158038147\n",
      "Testing Accuracy: 0.8260869565217391\n",
      "Training PR_AUC: 0.9077316615320846\n",
      "Testing PR_AUC: 0.9066362677117878\n",
      "Training Precision: [0.54632153 0.86298077 1.        ]\n",
      "Testing Precision: [0.58152174 0.90322581 1.        ]\n",
      "Training Recall: [1.         0.89526185 0.        ]\n",
      "Testing Recall: [1.         0.78504673 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=16,4\n",
      "Training Accuracy: 0.8732970027247956\n",
      "Testing Accuracy: 0.8260869565217391\n",
      "Training PR_AUC: 0.913700164683131\n",
      "Testing PR_AUC: 0.9117865100055245\n",
      "Training Precision: [0.54632153 0.87378641 1.        ]\n",
      "Testing Precision: [0.58152174 0.92134831 1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.         0.76635514 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=64,16\n",
      "Training Accuracy: 0.8746594005449592\n",
      "Testing Accuracy: 0.8206521739130435\n",
      "Training PR_AUC: 0.9140829068787603\n",
      "Testing PR_AUC: 0.9041548151158065\n",
      "Training Precision: [0.54632153 0.87228916 1.        ]\n",
      "Testing Precision: [0.58152174 0.90217391 1.        ]\n",
      "Training Recall: [1.         0.90274314 0.        ]\n",
      "Testing Recall: [1.         0.77570093 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=9,6\n",
      "Training Accuracy: 0.8678474114441417\n",
      "Testing Accuracy: 0.8260869565217391\n",
      "Training PR_AUC: 0.908039136082859\n",
      "Testing PR_AUC: 0.9043472914305268\n",
      "Training Precision: [0.54632153 0.85680751 1.        ]\n",
      "Testing Precision: [0.58152174 0.89473684 1.        ]\n",
      "Training Recall: [1.         0.91022444 0.        ]\n",
      "Testing Recall: [1.         0.79439252 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=27,18\n",
      "Training Accuracy: 0.8732970027247956\n",
      "Testing Accuracy: 0.8097826086956522\n",
      "Training PR_AUC: 0.9130344824116611\n",
      "Testing PR_AUC: 0.8967645266151971\n",
      "Training Precision: [0.54632153 0.87019231 1.        ]\n",
      "Testing Precision: [0.58152174 0.89130435 1.        ]\n",
      "Training Recall: [1.         0.90274314 0.        ]\n",
      "Testing Recall: [1.         0.76635514 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance\n",
      "Training Accuracy: 0.8732970027247956\n",
      "Testing Accuracy: 0.8206521739130435\n",
      "Training PR_AUC: 0.9130344824116611\n",
      "Testing PR_AUC: 0.9041548151158065\n",
      "Training Precision: [0.54632153 0.87019231 1.        ]\n",
      "Testing Precision: [0.58152174 0.90217391 1.        ]\n",
      "Training Recall: [1.         0.90274314 0.        ]\n",
      "Testing Recall: [1.         0.77570093 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=200\n",
      "Training Accuracy: 0.8746594005449592\n",
      "Testing Accuracy: 0.8152173913043478\n",
      "Training PR_AUC: 0.9140829068787603\n",
      "Testing PR_AUC: 0.8993044177338524\n",
      "Training Precision: [0.54632153 0.87228916 1.        ]\n",
      "Testing Precision: [0.58152174 0.89247312 1.        ]\n",
      "Training Recall: [1.         0.90274314 0.        ]\n",
      "Testing Recall: [1.         0.77570093 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300\n",
      "Training Accuracy: 0.8746594005449592\n",
      "Testing Accuracy: 0.8152173913043478\n",
      "Training PR_AUC: 0.9140829068787603\n",
      "Testing PR_AUC: 0.8993044177338524\n",
      "Training Precision: [0.54632153 0.87228916 1.        ]\n",
      "Testing Precision: [0.58152174 0.89247312 1.        ]\n",
      "Training Recall: [1.         0.90274314 0.        ]\n",
      "Testing Recall: [1.         0.77570093 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300 min_samples_split=4\n",
      "Training Accuracy: 0.8746594005449592\n",
      "Testing Accuracy: 0.8152173913043478\n",
      "Training PR_AUC: 0.9144186495251866\n",
      "Testing PR_AUC: 0.8993044177338524\n",
      "Training Precision: [0.54632153 0.87409201 1.        ]\n",
      "Testing Precision: [0.58152174 0.89247312 1.        ]\n",
      "Training Recall: [1.         0.90024938 0.        ]\n",
      "Testing Recall: [1.         0.77570093 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300 min_samples_split=7\n",
      "Training Accuracy: 0.8746594005449592\n",
      "Testing Accuracy: 0.8152173913043478\n",
      "Training PR_AUC: 0.9140829068787603\n",
      "Testing PR_AUC: 0.8993044177338524\n",
      "Training Precision: [0.54632153 0.87228916 1.        ]\n",
      "Testing Precision: [0.58152174 0.89247312 1.        ]\n",
      "Training Recall: [1.         0.90274314 0.        ]\n",
      "Testing Recall: [1.         0.77570093 0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Putting in separate cells because results is too long\n",
    "e = 6\n",
    "print(f\"Experiment {e+1}:\")\n",
    "experiment(experiments_train[e], y_train, experiments_test[e], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 8:\n",
      "LogisticRegression: solver=lbfgs\n",
      "Training Accuracy: 0.7806539509536785\n",
      "Testing Accuracy: 0.75\n",
      "Training PR_AUC: 0.8551973399136825\n",
      "Testing PR_AUC: 0.8483059538321627\n",
      "Training Precision: [0.54632153 0.80769231 1.        ]\n",
      "Testing Precision: [0.58152174 0.79047619 1.        ]\n",
      "Training Recall: [1.         0.78553616 0.        ]\n",
      "Testing Recall: [1.         0.77570093 0.        ]\n",
      "\n",
      "LogisticRegression: solver=newton-cg\n",
      "Training Accuracy: 0.7806539509536785\n",
      "Testing Accuracy: 0.75\n",
      "Training PR_AUC: 0.8551973399136825\n",
      "Testing PR_AUC: 0.8483059538321627\n",
      "Training Precision: [0.54632153 0.80769231 1.        ]\n",
      "Testing Precision: [0.58152174 0.79047619 1.        ]\n",
      "Training Recall: [1.         0.78553616 0.        ]\n",
      "Testing Recall: [1.         0.77570093 0.        ]\n",
      "\n",
      "LogisticRegression: solver=liblinear\n",
      "Training Accuracy: 0.782016348773842\n",
      "Testing Accuracy: 0.75\n",
      "Training PR_AUC: 0.8562355048335957\n",
      "Testing PR_AUC: 0.8483059538321627\n",
      "Training Precision: [0.54632153 0.80976864 1.        ]\n",
      "Testing Precision: [0.58152174 0.79047619 1.        ]\n",
      "Training Recall: [1.         0.78553616 0.        ]\n",
      "Testing Recall: [1.         0.77570093 0.        ]\n",
      "\n",
      "LogisticRegression: solver=lbfgs & penalty=l2\n",
      "Training Accuracy: 0.7806539509536785\n",
      "Testing Accuracy: 0.75\n",
      "Training PR_AUC: 0.8551973399136825\n",
      "Testing PR_AUC: 0.8483059538321627\n",
      "Training Precision: [0.54632153 0.80769231 1.        ]\n",
      "Testing Precision: [0.58152174 0.79047619 1.        ]\n",
      "Training Recall: [1.         0.78553616 0.        ]\n",
      "Testing Recall: [1.         0.77570093 0.        ]\n",
      "\n",
      "LogisticRegression: solver=lbfgs & penalty=none\n",
      "Training Accuracy: 0.7806539509536785\n",
      "Testing Accuracy: 0.75\n",
      "Training PR_AUC: 0.8551973399136825\n",
      "Testing PR_AUC: 0.8483059538321627\n",
      "Training Precision: [0.54632153 0.80769231 1.        ]\n",
      "Testing Precision: [0.58152174 0.79047619 1.        ]\n",
      "Training Recall: [1.         0.78553616 0.        ]\n",
      "Testing Recall: [1.         0.77570093 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.717391304347826\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8265440877691995\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.73913043 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.79439252 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=entropy\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.7010869565217391\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8171837812735822\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.73214286 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.76635514 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini splitter=best\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.6739130434782609\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.801539703129504\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.7008547  1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.76635514 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini splitter=random\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.6956521739130435\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8140217116751046\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.72972973 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.75700935 0.        ]\n",
      "\n",
      "MLPClassifier: activation=adam solver=relu\n",
      "Training Accuracy: 0.9318801089918256\n",
      "Testing Accuracy: 0.7228260869565217\n",
      "Training PR_AUC: 0.9605160247271713\n",
      "Testing PR_AUC: 0.8312323185083529\n",
      "Training Precision: [0.54632153 0.96062992 1.        ]\n",
      "Testing Precision: [0.58152174 0.76415094 1.        ]\n",
      "Training Recall: [1.        0.9127182 0.       ]\n",
      "Testing Recall: [1.         0.75700935 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=4,1\n",
      "Training Accuracy: 0.7942779291553134\n",
      "Testing Accuracy: 0.7663043478260869\n",
      "Training PR_AUC: 0.867441685571769\n",
      "Testing PR_AUC: 0.8638680075849926\n",
      "Training Precision: [0.54632153 0.83783784 1.        ]\n",
      "Testing Precision: [0.58152174 0.83333333 1.        ]\n",
      "Training Recall: [1.         0.77306733 0.        ]\n",
      "Testing Recall: [1.         0.74766355 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=16,4\n",
      "Training Accuracy: 0.7970027247956403\n",
      "Testing Accuracy: 0.7663043478260869\n",
      "Training PR_AUC: 0.8668263568324321\n",
      "Testing PR_AUC: 0.8599306036921066\n",
      "Training Precision: [0.54632153 0.82642487 1.        ]\n",
      "Testing Precision: [0.58152174 0.81372549 1.        ]\n",
      "Training Recall: [1.         0.79551122 0.        ]\n",
      "Testing Recall: [1.         0.77570093 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=64,16\n",
      "Training Accuracy: 0.9822888283378747\n",
      "Testing Accuracy: 0.6684782608695652\n",
      "Training PR_AUC: 0.9891931219013385\n",
      "Testing PR_AUC: 0.7980007345356797\n",
      "Training Precision: [0.54632153 0.98743719 1.        ]\n",
      "Testing Precision: [0.58152174 0.72115385 1.        ]\n",
      "Training Recall: [1.         0.98004988 0.        ]\n",
      "Testing Recall: [1.         0.70093458 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=9,6\n",
      "Training Accuracy: 0.7983651226158038\n",
      "Testing Accuracy: 0.75\n",
      "Training PR_AUC: 0.8697908638780648\n",
      "Testing PR_AUC: 0.8524623298717728\n",
      "Training Precision: [0.54632153 0.83914209 1.        ]\n",
      "Testing Precision: [0.58152174 0.81443299 1.        ]\n",
      "Training Recall: [1.         0.78054863 0.        ]\n",
      "Testing Recall: [1.         0.73831776 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=27,18\n",
      "Training Accuracy: 0.9591280653950953\n",
      "Testing Accuracy: 0.6956521739130435\n",
      "Training PR_AUC: 0.9752570952827455\n",
      "Testing PR_AUC: 0.8144047135310849\n",
      "Training Precision: [0.54632153 0.97201018 1.        ]\n",
      "Testing Precision: [0.58152174 0.73831776 1.        ]\n",
      "Training Recall: [1.         0.95261845 0.        ]\n",
      "Testing Recall: [1.         0.73831776 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance\n",
      "Training Accuracy: 0.8705722070844687\n",
      "Testing Accuracy: 0.7554347826086957\n",
      "Training PR_AUC: 0.9157374993118328\n",
      "Testing PR_AUC: 0.8553646062244483\n",
      "Training Precision: [0.54632153 0.89030612 1.        ]\n",
      "Testing Precision: [0.58152174 0.81632653 1.        ]\n",
      "Training Recall: [1.         0.87032419 0.        ]\n",
      "Testing Recall: [1.         0.74766355 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=200\n",
      "Training Accuracy: 0.9359673024523161\n",
      "Testing Accuracy: 0.7445652173913043\n",
      "Training PR_AUC: 0.959926157400249\n",
      "Testing PR_AUC: 0.848307059516208\n",
      "Training Precision: [0.54632153 0.95153061 1.        ]\n",
      "Testing Precision: [0.58152174 0.80612245 1.        ]\n",
      "Training Recall: [1.         0.93017456 0.        ]\n",
      "Testing Recall: [1.         0.73831776 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300\n",
      "Training Accuracy: 0.9713896457765667\n",
      "Testing Accuracy: 0.7228260869565217\n",
      "Training PR_AUC: 0.9825450994208256\n",
      "Testing PR_AUC: 0.8325007369871963\n",
      "Training Precision: [0.54632153 0.97979798 1.        ]\n",
      "Testing Precision: [0.58152174 0.7745098  1.        ]\n",
      "Training Recall: [1.         0.96758105 0.        ]\n",
      "Testing Recall: [1.         0.73831776 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300 min_samples_split=4\n",
      "Training Accuracy: 0.9673024523160763\n",
      "Testing Accuracy: 0.7336956521739131\n",
      "Training PR_AUC: 0.9801223366926731\n",
      "Testing PR_AUC: 0.8412495128079676\n",
      "Training Precision: [0.54632153 0.97721519 1.        ]\n",
      "Testing Precision: [0.58152174 0.79591837 1.        ]\n",
      "Training Recall: [1.         0.96259352 0.        ]\n",
      "Testing Recall: [1.         0.72897196 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300 min_samples_split=7\n",
      "Training Accuracy: 0.9673024523160763\n",
      "Testing Accuracy: 0.717391304347826\n",
      "Training PR_AUC: 0.9807709408246825\n",
      "Testing PR_AUC: 0.8302237121314732\n",
      "Training Precision: [0.54632153 0.97964377 1.        ]\n",
      "Testing Precision: [0.58152174 0.77777778 1.        ]\n",
      "Training Recall: [1.         0.96009975 0.        ]\n",
      "Testing Recall: [1.         0.71962617 0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Putting in separate cells because results is too long\n",
    "e = 7\n",
    "print(f\"Experiment {e+1}:\")\n",
    "experiment(experiments_train[e], y_train, experiments_test[e], y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 9:\n",
      "LogisticRegression: solver=lbfgs\n",
      "Training Accuracy: 0.8610354223433242\n",
      "Testing Accuracy: 0.8804347826086957\n",
      "Training PR_AUC: 0.9043604049801583\n",
      "Testing PR_AUC: 0.9198908962724486\n",
      "Training Precision: [0.54632153 0.85510689 1.        ]\n",
      "Testing Precision: [0.58152174 0.86324786 1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.         0.94392523 0.        ]\n",
      "\n",
      "LogisticRegression: solver=newton-cg\n",
      "Training Accuracy: 0.8610354223433242\n",
      "Testing Accuracy: 0.8804347826086957\n",
      "Training PR_AUC: 0.9043604049801583\n",
      "Testing PR_AUC: 0.9198908962724486\n",
      "Training Precision: [0.54632153 0.85510689 1.        ]\n",
      "Testing Precision: [0.58152174 0.86324786 1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.         0.94392523 0.        ]\n",
      "\n",
      "LogisticRegression: solver=liblinear\n",
      "Training Accuracy: 0.8610354223433242\n",
      "Testing Accuracy: 0.875\n",
      "Training PR_AUC: 0.9040865926731152\n",
      "Testing PR_AUC: 0.9185664791804786\n",
      "Training Precision: [0.54632153 0.8534279  1.        ]\n",
      "Testing Precision: [0.58152174 0.86842105 1.        ]\n",
      "Training Recall: [1.         0.90024938 0.        ]\n",
      "Testing Recall: [1.         0.92523364 0.        ]\n",
      "\n",
      "LogisticRegression: solver=lbfgs & penalty=l2\n",
      "Training Accuracy: 0.8610354223433242\n",
      "Testing Accuracy: 0.8804347826086957\n",
      "Training PR_AUC: 0.9043604049801583\n",
      "Testing PR_AUC: 0.9198908962724486\n",
      "Training Precision: [0.54632153 0.85510689 1.        ]\n",
      "Testing Precision: [0.58152174 0.86324786 1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.         0.94392523 0.        ]\n",
      "\n",
      "LogisticRegression: solver=lbfgs & penalty=none\n",
      "Training Accuracy: 0.8610354223433242\n",
      "Testing Accuracy: 0.875\n",
      "Training PR_AUC: 0.9043604049801583\n",
      "Testing PR_AUC: 0.9173459415152238\n",
      "Training Precision: [0.54632153 0.85510689 1.        ]\n",
      "Testing Precision: [0.58152174 0.86206897 1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.         0.93457944 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini\n",
      "Training Accuracy: 0.8746594005449592\n",
      "Testing Accuracy: 0.8260869565217391\n",
      "Training PR_AUC: 0.9147631651791979\n",
      "Testing PR_AUC: 0.9043472914305268\n",
      "Training Precision: [0.54632153 0.87591241 1.        ]\n",
      "Testing Precision: [0.58152174 0.89473684 1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.         0.79439252 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=entropy\n",
      "Training Accuracy: 0.8746594005449592\n",
      "Testing Accuracy: 0.8315217391304348\n",
      "Training PR_AUC: 0.9147631651791979\n",
      "Testing PR_AUC: 0.9068510429364758\n",
      "Training Precision: [0.54632153 0.87591241 1.        ]\n",
      "Testing Precision: [0.58152174 0.89583333 1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.         0.80373832 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini splitter=best\n",
      "Training Accuracy: 0.8746594005449592\n",
      "Testing Accuracy: 0.8260869565217391\n",
      "Training PR_AUC: 0.9147631651791979\n",
      "Testing PR_AUC: 0.9043472914305268\n",
      "Training Precision: [0.54632153 0.87591241 1.        ]\n",
      "Testing Precision: [0.58152174 0.89473684 1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.         0.79439252 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini splitter=random\n",
      "Training Accuracy: 0.8746594005449592\n",
      "Testing Accuracy: 0.8260869565217391\n",
      "Training PR_AUC: 0.9147631651791979\n",
      "Testing PR_AUC: 0.8915418364321248\n",
      "Training Precision: [0.54632153 0.87591241 1.        ]\n",
      "Testing Precision: [0.58152174 0.83783784 1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.         0.86915888 0.        ]\n",
      "\n",
      "MLPClassifier: activation=adam solver=relu\n",
      "Training Accuracy: 0.8610354223433242\n",
      "Testing Accuracy: 0.8695652173913043\n",
      "Training PR_AUC: 0.9043604049801583\n",
      "Testing PR_AUC: 0.9173818780177984\n",
      "Training Precision: [0.54632153 0.85510689 1.        ]\n",
      "Testing Precision: [0.58152174 0.87387387 1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.         0.90654206 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=4,1\n",
      "Training Accuracy: 0.5463215258855586\n",
      "Testing Accuracy: 0.5815217391304348\n",
      "Training PR_AUC: 0.7731607629427792\n",
      "Testing PR_AUC: 0.7907608695652174\n",
      "Training Precision: [0.54632153 1.        ]\n",
      "Testing Precision: [0.58152174 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1. 0.]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=16,4\n",
      "Training Accuracy: 0.8583106267029973\n",
      "Testing Accuracy: 0.875\n",
      "Training PR_AUC: 0.901580935525366\n",
      "Testing PR_AUC: 0.9199004469727753\n",
      "Training Precision: [0.54632153 0.84615385 1.        ]\n",
      "Testing Precision: [0.58152174 0.875      1.        ]\n",
      "Training Recall: [1.         0.90523691 0.        ]\n",
      "Testing Recall: [1.         0.91588785 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=64,16\n",
      "Training Accuracy: 0.8610354223433242\n",
      "Testing Accuracy: 0.875\n",
      "Training PR_AUC: 0.9043604049801583\n",
      "Testing PR_AUC: 0.9199004469727753\n",
      "Training Precision: [0.54632153 0.85510689 1.        ]\n",
      "Testing Precision: [0.58152174 0.875      1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.         0.91588785 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=9,6\n",
      "Training Accuracy: 0.8610354223433242\n",
      "Testing Accuracy: 0.8641304347826086\n",
      "Training PR_AUC: 0.9040865926731152\n",
      "Testing PR_AUC: 0.910040272315925\n",
      "Training Precision: [0.54632153 0.8534279  1.        ]\n",
      "Testing Precision: [0.58152174 0.84745763 1.        ]\n",
      "Training Recall: [1.         0.90024938 0.        ]\n",
      "Testing Recall: [1.         0.93457944 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=27,18\n",
      "Training Accuracy: 0.8610354223433242\n",
      "Testing Accuracy: 0.8804347826086957\n",
      "Training PR_AUC: 0.9043604049801583\n",
      "Testing PR_AUC: 0.9198908962724486\n",
      "Training Precision: [0.54632153 0.85510689 1.        ]\n",
      "Testing Precision: [0.58152174 0.86324786 1.        ]\n",
      "Training Recall: [1.         0.89775561 0.        ]\n",
      "Testing Recall: [1.         0.94392523 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance\n",
      "Training Accuracy: 0.8678474114441417\n",
      "Testing Accuracy: 0.8097826086956522\n",
      "Training PR_AUC: 0.9088907095178882\n",
      "Testing PR_AUC: 0.888934376269809\n",
      "Training Precision: [0.54632153 0.86190476 1.        ]\n",
      "Testing Precision: [0.58152174 0.86       1.        ]\n",
      "Training Recall: [1.         0.90274314 0.        ]\n",
      "Testing Recall: [1.         0.80373832 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=200\n",
      "Training Accuracy: 0.8732970027247956\n",
      "Testing Accuracy: 0.8043478260869565\n",
      "Training PR_AUC: 0.9130344824116611\n",
      "Testing PR_AUC: 0.8862717996708245\n",
      "Training Precision: [0.54632153 0.87019231 1.        ]\n",
      "Testing Precision: [0.58152174 0.85858586 1.        ]\n",
      "Training Recall: [1.         0.90274314 0.        ]\n",
      "Testing Recall: [1.         0.79439252 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300\n",
      "Training Accuracy: 0.8732970027247956\n",
      "Testing Accuracy: 0.8043478260869565\n",
      "Training PR_AUC: 0.9130344824116611\n",
      "Testing PR_AUC: 0.8862717996708245\n",
      "Training Precision: [0.54632153 0.87019231 1.        ]\n",
      "Testing Precision: [0.58152174 0.85858586 1.        ]\n",
      "Training Recall: [1.         0.90274314 0.        ]\n",
      "Testing Recall: [1.         0.79439252 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300 min_samples_split=4\n",
      "Training Accuracy: 0.8732970027247956\n",
      "Testing Accuracy: 0.8043478260869565\n",
      "Training PR_AUC: 0.9130344824116611\n",
      "Testing PR_AUC: 0.8862717996708245\n",
      "Training Precision: [0.54632153 0.87019231 1.        ]\n",
      "Testing Precision: [0.58152174 0.85858586 1.        ]\n",
      "Training Recall: [1.         0.90274314 0.        ]\n",
      "Testing Recall: [1.         0.79439252 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300 min_samples_split=7\n",
      "Training Accuracy: 0.8732970027247956\n",
      "Testing Accuracy: 0.8043478260869565\n",
      "Training PR_AUC: 0.9130344824116611\n",
      "Testing PR_AUC: 0.8862717996708245\n",
      "Training Precision: [0.54632153 0.87019231 1.        ]\n",
      "Testing Precision: [0.58152174 0.85858586 1.        ]\n",
      "Training Recall: [1.         0.90274314 0.        ]\n",
      "Testing Recall: [1.         0.79439252 0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Putting in separate cells because results is too long\n",
    "e = 8\n",
    "print(f\"Experiment {e+1}:\")\n",
    "experiment(experiments_train[e], y_train, experiments_test[e], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 10:\n",
      "LogisticRegression: solver=lbfgs\n",
      "Training Accuracy: 0.784741144414169\n",
      "Testing Accuracy: 0.7228260869565217\n",
      "Training PR_AUC: 0.8561067161147848\n",
      "Testing PR_AUC: 0.8406482028739242\n",
      "Training Precision: [0.54632153 0.79706601 1.        ]\n",
      "Testing Precision: [0.58152174 0.81818182 1.        ]\n",
      "Training Recall: [1.         0.81296758 0.        ]\n",
      "Testing Recall: [1.        0.6728972 0.       ]\n",
      "\n",
      "LogisticRegression: solver=newton-cg\n",
      "Training Accuracy: 0.784741144414169\n",
      "Testing Accuracy: 0.7228260869565217\n",
      "Training PR_AUC: 0.8561067161147848\n",
      "Testing PR_AUC: 0.8406482028739242\n",
      "Training Precision: [0.54632153 0.79706601 1.        ]\n",
      "Testing Precision: [0.58152174 0.81818182 1.        ]\n",
      "Training Recall: [1.         0.81296758 0.        ]\n",
      "Testing Recall: [1.        0.6728972 0.       ]\n",
      "\n",
      "LogisticRegression: solver=liblinear\n",
      "Training Accuracy: 0.784741144414169\n",
      "Testing Accuracy: 0.717391304347826\n",
      "Training PR_AUC: 0.8559496116408596\n",
      "Testing PR_AUC: 0.8346117007738301\n",
      "Training Precision: [0.54632153 0.79562044 1.        ]\n",
      "Testing Precision: [0.58152174 0.8021978  1.        ]\n",
      "Training Recall: [1.         0.81546135 0.        ]\n",
      "Testing Recall: [1.         0.68224299 0.        ]\n",
      "\n",
      "LogisticRegression: solver=lbfgs & penalty=l2\n",
      "Training Accuracy: 0.784741144414169\n",
      "Testing Accuracy: 0.7228260869565217\n",
      "Training PR_AUC: 0.8561067161147848\n",
      "Testing PR_AUC: 0.8406482028739242\n",
      "Training Precision: [0.54632153 0.79706601 1.        ]\n",
      "Testing Precision: [0.58152174 0.81818182 1.        ]\n",
      "Training Recall: [1.         0.81296758 0.        ]\n",
      "Testing Recall: [1.        0.6728972 0.       ]\n",
      "\n",
      "LogisticRegression: solver=lbfgs & penalty=none\n",
      "Training Accuracy: 0.7806539509536785\n",
      "Testing Accuracy: 0.6956521739130435\n",
      "Training PR_AUC: 0.8551973399136825\n",
      "Testing PR_AUC: 0.8292611576029092\n",
      "Training Precision: [0.54632153 0.80769231 1.        ]\n",
      "Testing Precision: [0.58152174 0.82278481 1.        ]\n",
      "Training Recall: [1.         0.78553616 0.        ]\n",
      "Testing Recall: [1.         0.60747664 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.6630434782608695\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.7948989382887902\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.72277228 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.68224299 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=entropy\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.6793478260869565\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8045423704123955\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.72641509 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.71962617 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini splitter=best\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.6793478260869565\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.8045423704123955\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.72641509 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.71962617 0.        ]\n",
      "\n",
      "DecisionTreeClassifier: criterion=gini splitter=random\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.6739130434782609\n",
      "Training PR_AUC: 1.0\n",
      "Testing PR_AUC: 0.803409567201093\n",
      "Training Precision: [1. 1.]\n",
      "Testing Precision: [0.58152174 0.68503937 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1.         0.81308411 0.        ]\n",
      "\n",
      "MLPClassifier: activation=adam solver=relu\n",
      "Training Accuracy: 0.8038147138964578\n",
      "Testing Accuracy: 0.7554347826086957\n",
      "Training PR_AUC: 0.8722490922029462\n",
      "Testing PR_AUC: 0.8581839461557748\n",
      "Training Precision: [0.54632153 0.83727034 1.        ]\n",
      "Testing Precision: [0.58152174 0.82978723 1.        ]\n",
      "Training Recall: [1.         0.79551122 0.        ]\n",
      "Testing Recall: [1.         0.72897196 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=4,1\n",
      "Training Accuracy: 0.5463215258855586\n",
      "Testing Accuracy: 0.5815217391304348\n",
      "Training PR_AUC: 0.7731607629427792\n",
      "Testing PR_AUC: 0.7907608695652174\n",
      "Training Precision: [0.54632153 1.        ]\n",
      "Testing Precision: [0.58152174 1.        ]\n",
      "Training Recall: [1. 0.]\n",
      "Testing Recall: [1. 0.]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=16,4\n",
      "Training Accuracy: 0.7779291553133515\n",
      "Testing Accuracy: 0.75\n",
      "Training PR_AUC: 0.8523639215992715\n",
      "Testing PR_AUC: 0.8538166449239718\n",
      "Training Precision: [0.54632153 0.7975     1.        ]\n",
      "Testing Precision: [0.58152174 0.82105263 1.        ]\n",
      "Training Recall: [1.         0.79551122 0.        ]\n",
      "Testing Recall: [1.         0.72897196 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=64,16\n",
      "Training Accuracy: 0.8079019073569482\n",
      "Testing Accuracy: 0.7228260869565217\n",
      "Training PR_AUC: 0.8759205781418677\n",
      "Testing PR_AUC: 0.8423924855655198\n",
      "Training Precision: [0.54632153 0.84574468 1.        ]\n",
      "Testing Precision: [0.58152174 0.8255814  1.        ]\n",
      "Training Recall: [1.         0.79301746 0.        ]\n",
      "Testing Recall: [1.        0.6635514 0.       ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=9,6\n",
      "Training Accuracy: 0.7915531335149864\n",
      "Testing Accuracy: 0.7608695652173914\n",
      "Training PR_AUC: 0.861236880618279\n",
      "Testing PR_AUC: 0.8610353087106226\n",
      "Training Precision: [0.54632153 0.80845771 1.        ]\n",
      "Testing Precision: [0.58152174 0.83157895 1.        ]\n",
      "Training Recall: [1.         0.81047382 0.        ]\n",
      "Testing Recall: [1.         0.73831776 0.        ]\n",
      "\n",
      "MLPClassifier: activation=relu solver=adam hidden_layer_sizes=27,18\n",
      "Training Accuracy: 0.8038147138964578\n",
      "Testing Accuracy: 0.75\n",
      "Training PR_AUC: 0.8716284062482599\n",
      "Testing PR_AUC: 0.8587721260654982\n",
      "Training Precision: [0.54632153 0.83376623 1.        ]\n",
      "Testing Precision: [0.58152174 0.84269663 1.        ]\n",
      "Training Recall: [1.         0.80049875 0.        ]\n",
      "Testing Recall: [1.         0.70093458 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance\n",
      "Training Accuracy: 0.8705722070844687\n",
      "Testing Accuracy: 0.717391304347826\n",
      "Training PR_AUC: 0.9157374993118328\n",
      "Testing PR_AUC: 0.8458303972348381\n",
      "Training Precision: [0.54632153 0.89030612 1.        ]\n",
      "Testing Precision: [0.58152174 0.84810127 1.        ]\n",
      "Training Recall: [1.         0.87032419 0.        ]\n",
      "Testing Recall: [1.         0.62616822 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=200\n",
      "Training Accuracy: 0.9359673024523161\n",
      "Testing Accuracy: 0.717391304347826\n",
      "Training PR_AUC: 0.959926157400249\n",
      "Testing PR_AUC: 0.8542694110312659\n",
      "Training Precision: [0.54632153 0.95153061 1.        ]\n",
      "Testing Precision: [0.58152174 0.87671233 1.        ]\n",
      "Training Recall: [1.         0.93017456 0.        ]\n",
      "Testing Recall: [1.         0.59813084 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300\n",
      "Training Accuracy: 0.9713896457765667\n",
      "Testing Accuracy: 0.7065217391304348\n",
      "Training PR_AUC: 0.9825450994208256\n",
      "Testing PR_AUC: 0.8425799133143709\n",
      "Training Precision: [0.54632153 0.97979798 1.        ]\n",
      "Testing Precision: [0.58152174 0.85333333 1.        ]\n",
      "Training Recall: [1.         0.96758105 0.        ]\n",
      "Testing Recall: [1.         0.59813084 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300 min_samples_split=4\n",
      "Training Accuracy: 0.9673024523160763\n",
      "Testing Accuracy: 0.6902173913043478\n",
      "Training PR_AUC: 0.9801223366926731\n",
      "Testing PR_AUC: 0.8284314249663167\n",
      "Training Precision: [0.54632153 0.97721519 1.        ]\n",
      "Testing Precision: [0.58152174 0.82894737 1.        ]\n",
      "Training Recall: [1.         0.96259352 0.        ]\n",
      "Testing Recall: [1.         0.58878505 0.        ]\n",
      "\n",
      "GradientBoostClassifier: loss=deviance n_estimators=300 min_samples_split=7\n",
      "Training Accuracy: 0.9673024523160763\n",
      "Testing Accuracy: 0.7010869565217391\n",
      "Training PR_AUC: 0.9807709408246825\n",
      "Testing PR_AUC: 0.842557790419432\n",
      "Training Precision: [0.54632153 0.97964377 1.        ]\n",
      "Testing Precision: [0.58152174 0.86111111 1.        ]\n",
      "Training Recall: [1.         0.96009975 0.        ]\n",
      "Testing Recall: [1.         0.57943925 0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Putting in separate cells because results is too long\n",
    "e = 9\n",
    "print(f\"Experiment {e+1}:\")\n",
    "experiment(experiments_train[e], y_train, experiments_test[e], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# wandb.init(project=\"hw03\")\n",
    "# wandb.config = {\n",
    "#   \"learning_rate\": 0.001,\n",
    "#   \"epochs\": 100,\n",
    "#   \"batch_size\": 128\n",
    "# }\n",
    "# wandb.log(wandb.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
