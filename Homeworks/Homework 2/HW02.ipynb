{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1cd952a",
   "metadata": {},
   "source": [
    "# Homework 2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d574cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "TODO = \"****TODO****\"\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4628dab6",
   "metadata": {},
   "source": [
    "## Problem 1: Implementing Linear Regression\n",
    "\n",
    "Let's first implement multiple learning algorithms for linear regression. Remember our hypothesis function is\n",
    "\n",
    "$$h(x;w) = w_0 + w_1  x_1 + w_2 x_2 + \\cdots$$\n",
    "\n",
    "In this case we are going to be learning just a single dimensional linear regression\n",
    "\n",
    "$$h(x;w) = w_0 + w_1 x_1$$\n",
    "\n",
    "Prompts\n",
    "\n",
    "1.1: LinearRegression\n",
    "- 1.1.1: Learn the linear regression model using a scikit learn package: `sklearn.linear_model import LinearRegression`. This learning algorithm uses linear least squares for a closed-form solution. Create an instance and then run `LinearRegression().fit(...)`. Report the learned weights w=[w_0, w_1]. Hint: use classifier.coef_ and classifier.intercept_. \n",
    "- 1.1.2: Report the train mean-square-error and test mean-squared-error for LinearRegression.\n",
    "\n",
    "1.2: SGDRegressor\n",
    "- 1.2.1: Learn the linear regression model using a scikit learn package `sklearn.linear_model import SGDRegressor`. This learning algorithm uses stochastic gradient descent to solve linear regression. Create an instance and then run `SGDRegressor(alpha=10).fit(...)`. Report the learned weights w=[w_0, w_1]. Hint: use classifier.coef_ and classifier.intercept_. \n",
    "- 1.2.2: Report the train mean-square-error and test mean-squared-error for SGDRegressor.\n",
    "\n",
    "1.3: Least Squares\n",
    "- 1.3.1: Solve for w_0, w_1 using linear least squares. Hint consider using `np.linalg.lstsq`. Report the learned weights `w=[w_0, w_1]`. Don't forget to add the column of 1s to your X matrix.\n",
    "- 1.3.2: Report the train mean-square-error and test mean-squared-error.\n",
    "\n",
    "1.4: Implement Gradient Descent\n",
    "- 1.4.1: Finish implementation of `gradient_of_mean_squared_error` for linear regression. Print the average of the gradient for the training data for a weight vector of `w=[0, 0]`. (Hint: use `gradient_of_mean_squared_error(...).mean(axis=0)`)\n",
    "- 1.4.2: Finish implementation of `batch_gradient_descent_epoch` for linear regression. Print out the new weights after running this for a single iteration starting from `w=[0, 0]` with alpha=`10`)\n",
    "- 1.4.3: Run `500 epochs` (steps) of gradient descent starting from `w=[0, 0]` with `alpha=10`. Create a plot where the y-axis is the train MeanSquaredError and the x-axis the epoch #.\n",
    "- 1.4.4: Try various values of alpha, num_epochs, and initial values of w (note, see how the error plot changes with alpha. Consider trying various alphas at 10x apart: `[..., 1000, 100, 10, 1, 0.01, 0.001,...]`). Discuss what happens with various choices. What was the best alpha you found? How many epochs should you run? What is a good initialization for w?\n",
    "- 1.4.5: Report solution for `w=[w_0, w_1]` for the best run of gradient descent. \n",
    "- 1.4.6: Report the train mean-square-error and test mean-squared-error for these weights.\n",
    "\n",
    "1.5: Are the weights learned the same between all three approaches? Is the mean-squared-error the same? Discuss.\n",
    "\n",
    "1.6: Extra credit: Why did we apply `X = StandardScaler().fit_transform(X)` to our LotArea X feature? What happens if we remove it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "45afd93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to implement linear regression prediction and mean squared error.\n",
    "def linear_regression_predict(x: np.array, w: np.array) -> float:\n",
    "    return w[0] + x.dot(w[1:])\n",
    "\n",
    "def mean_squared_error(y_true, y_predicted) -> float:\n",
    "  n = len(y)\n",
    "  error = 0\n",
    "  for truth, prediction in zip(y_true, y_predicted):\n",
    "    absolute_error = truth - prediction\n",
    "    error += absolute_error * absolute_error\n",
    "  return error/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ca2d3ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the heart disease dataset:\n",
    "columns = ['LotArea', 'SalePrice']\n",
    "housing_prices_dataframe = pd.read_csv(\"../../data/housing_prices.csv\")[columns]\n",
    "housing_prices_dataframe['SalePrice'] /= 1000\n",
    "housing_prices_dataframe['SalePrice'] \n",
    "\n",
    "# Select out X and Y\n",
    "X = housing_prices_dataframe[['LotArea']].values\n",
    "y = housing_prices_dataframe['SalePrice'].values\n",
    "\n",
    "# Apply standard scaler to X.\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# =======================================================\n",
    "# Our train and test dataset for this problem.\n",
    "# =======================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7a68db26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution to 1.1.1: linear regression weights are: [180.90095813968003, array([17.9705058])]\n"
     ]
    }
   ],
   "source": [
    "# Question 1.1.1\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "print(f\"Solution to 1.1.1: linear regression weights are: {[lr.intercept_, lr.coef_]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f5c4b177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution to 1.1.2: Train MSE: 3921.823323480261, Test MSE; 1954.8256551478441\n"
     ]
    }
   ],
   "source": [
    "# Question to 1.1.2\n",
    "print(f\"Solution to 1.1.2: Train MSE: {mean_squared_error(y_train, lr.predict(X_train))}, Test MSE; {mean_squared_error(y_test, lr.predict(X_test))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "40f7baf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution to 1.2.1: weights are: [array([181.31703216]), array([3.4115611])]\n"
     ]
    }
   ],
   "source": [
    "# Question 1.2.1\n",
    "sgd = SGDRegressor(alpha=10).fit(X_train, y_train)\n",
    "print(f\"Solution to 1.2.1: weights are: {[sgd.intercept_, sgd.coef_]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b9fc71bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution to 1.2.2: Train MSE: 3922.193009001176, Test MSE; 1958.5537958888472\n"
     ]
    }
   ],
   "source": [
    "# Question to 1.2.2\n",
    "print(f\"Solution to 1.2.2: Train MSE: {mean_squared_error(y_train, sgd.predict(X_train))}, Test MSE; {mean_squared_error(y_test, sgd.predict(X_test))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b9cd6176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution to 1.3.1: (array([180.90095814,  17.9705058 ]), array([5725862.05228119]), 2, array([36.31528008, 31.93982835]))\n"
     ]
    }
   ],
   "source": [
    "# Question to 1.3.1\n",
    "# Add a column of 1s to make this easier:\n",
    "X_1s_augmented = np.hstack([np.ones(X_train.shape[0]).reshape(-1, 1), X_train])\n",
    "y_1s = np.linalg.lstsq(X_1s_augmented, y_train, rcond=-1)\n",
    "# TODO implement least squares\n",
    "print(f\"Solution to 1.3.1: {y_1s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e2198d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution to 1.3.2: Train MSE: [2.24546821e+10 2.24546821e+10], Test MSE; (array([154.5  , 325.   , 115.   , 159.   , 315.5  ,  75.5  , 311.5  ,\n",
      "       146.   ,  84.5  , 135.5  , 145.   , 130.   ,  81.   , 214.   ,\n",
      "       181.   , 134.5  , 183.5  , 135.   , 118.4  , 226.   , 155.   ,\n",
      "       210.   , 173.5  , 129.   , 192.   , 153.9  , 181.134, 141.   ,\n",
      "       181.   , 208.9  , 127.   , 284.   , 200.5  , 135.75 , 255.   ,\n",
      "       140.   , 138.   , 219.5  , 310.   ,  97.   , 114.5  , 205.   ,\n",
      "       119.5  , 253.293, 128.5  , 117.5  , 115.   , 127.   , 451.95 ,\n",
      "       144.   , 119.   , 196.   , 115.   , 287.   , 144.5  , 260.   ,\n",
      "       213.   , 175.   , 107.   , 107.5  ,  68.5  , 154.   , 317.   ,\n",
      "       264.132, 283.463, 243.   , 109.   , 305.   ,  93.5  , 176.   ,\n",
      "       118.858, 134.   , 109.008,  93.5  , 611.657, 173.   , 348.   ,\n",
      "       341.   , 141.   , 124.9  , 118.   ,  67.   , 113.   ,  91.3  ,\n",
      "       149.5  , 133.   , 266.   , 190.   , 155.9  , 155.835, 153.5  ,\n",
      "       152.   , 124.5  , 301.   , 136.5  , 169.99 , 205.   , 183.9  ,\n",
      "       204.9  , 260.   , 163.5  , 224.9  , 244.   , 132.   , 194.   ,\n",
      "       156.5  , 156.   , 275.   , 145.   , 135.   ,  60.   , 124.   ,\n",
      "       127.   , 137.5  , 213.5  , 119.   , 107.9  , 123.   , 112.   ,\n",
      "       284.   , 133.   , 149.   , 169.   , 207.   , 175.   , 137.   ,\n",
      "       236.   ,  79.5  , 144.   , 162.9  , 185.9  , 369.9  , 197.9  ,\n",
      "       104.   ,  35.311, 337.5  , 367.294, 130.25 , 230.   , 755.   ,\n",
      "       403.   , 132.   , 178.   , 136.5  , 145.   , 123.   , 250.   ,\n",
      "       187.1  , 133.9  ,  67.   , 137.5  , 155.   , 200.624, 154.3  ,\n",
      "        91.   , 136.   , 108.959, 140.   ,  86.   , 131.4  , 179.9  ,\n",
      "       144.   , 293.077, 144.5  , 118.5  , 141.   , 239.   , 276.   ,\n",
      "       556.581, 244.4  , 360.   , 103.2  , 102.   , 151.   , 285.   ,\n",
      "       134.432, 113.   , 187.5  , 125.5  , 177.5  , 179.9  ,  55.993,\n",
      "       132.5  , 135.   , 255.   , 140.   , 271.   , 246.578, 202.5  ,\n",
      "        75.   , 122.5  , 108.48 , 160.   , 171.   , 196.   , 225.   ,\n",
      "       197.   ,  40.   , 172.5  , 154.9  , 280.   , 175.   , 147.   ,\n",
      "       315.   , 185.   , 135.5  , 239.5  , 139.   , 140.   , 110.   ,\n",
      "       225.   , 143.5  , 128.95 , 172.5  , 241.5  , 262.5  , 194.201,\n",
      "       143.   , 130.   , 126.   , 142.5  , 254.   , 217.5  ,  66.5  ,\n",
      "       201.   , 155.   ,  68.4  ,  64.5  , 173.   , 102.776,  84.9  ,\n",
      "       165.6  , 120.   , 135.   , 220.   , 153.575, 195.4  , 147.   ,\n",
      "       277.   , 143.   , 105.9  , 242.   , 194.5  , 438.78 , 185.   ,\n",
      "       107.5  , 165.   , 176.   , 129.9  , 115.   , 192.14 , 160.   ,\n",
      "       145.   ,  86.   , 158.   , 127.5  , 115.   , 119.5  , 175.9  ,\n",
      "       240.   , 395.   , 165.   , 128.2  , 275.   , 311.872, 214.   ,\n",
      "       153.5  , 144.   , 115.   , 180.   , 465.   , 180.   , 253.   ,\n",
      "        85.   , 101.8  , 148.5  , 137.5  , 318.061, 143.   , 140.   ,\n",
      "       192.5  ,  92.   , 197.   , 109.5  , 297.   , 185.75 , 230.   ,\n",
      "        89.471, 260.   , 189.   , 108.   , 124.5  , 145.   , 178.   ,\n",
      "        85.   , 175.   , 127.   , 149.9  , 174.   , 125.5  , 175.5  ,\n",
      "       225.   , 129.   , 159.95 , 157.   , 205.   , 140.   , 200.   ,\n",
      "       217.   , 125.   , 159.5  , 184.   ,  96.5  , 200.   , 149.   ,\n",
      "       178.9  , 184.9  , 164.7  , 335.   ,  87.5  , 233.17 , 149.35 ,\n",
      "       133.   ,  98.   , 207.5  , 150.   , 134.9  , 187.75 , 149.5  ,\n",
      "       116.05 , 153.337, 165.   , 130.5  , 206.3  , 146.   , 126.5  ,\n",
      "       139.   , 113.   ,  79.   , 182.   , 188.   , 129.   , 160.   ,\n",
      "       174.   , 219.21 , 310.   , 374.   , 100.   , 250.   , 145.   ,\n",
      "       325.   , 380.   , 275.   , 180.   , 245.35 , 151.5  , 134.5  ,\n",
      "        94.   , 216.   , 350.   , 195.   , 120.   , 228.5  , 248.   ,\n",
      "       124.   , 191.   , 181.   , 105.   , 139.9  , 157.9  , 130.5  ,\n",
      "       172.4  , 130.   , 178.   , 161.5  , 119.9  , 239.   , 190.   ,\n",
      "        85.4  , 205.   , 134.   , 168.   , 185.   , 180.5  , 152.   ,\n",
      "       215.   , 153.   , 106.   , 340.   , 159.   , 120.   , 115.   ,\n",
      "       128.   , 314.813, 131.   , 446.261, 127.5  , 155.   , 177.5  ,\n",
      "       128.5  , 176.   , 402.   , 130.   , 145.   , 147.   , 115.   ,\n",
      "       189.   , 143.   , 240.   , 230.   , 190.   , 213.   ,  82.5  ,\n",
      "       274.9  , 155.   , 423.   , 128.5  , 335.   , 262.   , 129.5  ,\n",
      "       222.5  , 270.   , 207.5  , 175.   , 238.   , 135.9  , 224.   ,\n",
      "       170.   , 185.   , 263.   ,  62.383, 150.   , 171.   , 139.   ,\n",
      "       126.175, 205.95 , 110.   , 485.   ]), (array([180.90095814,  17.9705058 ]), array([5725862.05228119]), 2, array([36.31528008, 31.93982835])))\n"
     ]
    }
   ],
   "source": [
    "# Question to 1.3.2\n",
    "print(f\"Solution to 1.3.2: Train MSE: {mean_squared_error(y_train, y_1s)}, Test MSE; {y_test, y_1s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9be304",
   "metadata": {},
   "source": [
    "### Gradient Descent for Linear Regression\n",
    "\n",
    "Remember, on each update for gradient descent we need to update our weights as follows:\n",
    "$$w_i^{(k+1)}:= w_i^k - \\alpha \\dfrac{∂}{∂w^{k}} J(w^{k})$$\n",
    "\n",
    "The algorithm then iteratively updates the weights as follows:\n",
    "$$\n",
    "temp_0 := w_0 - \\alpha \\dfrac{∂}{∂{w_0}} J(w)\\\\\n",
    "temp_1 := w_1 - \\alpha \\dfrac{∂}{∂{w_1}} J(w) \\\\\n",
    "w_0 := temp_0 \\\\\n",
    "w_1 := temp_1\n",
    "$$\n",
    "\n",
    "To implement `gradient_of_mean_squared_error` you will need to implement this gradient for each sample and then `batch_gradient_descent_epoch` will sum them up. Remember this is the gradient calculation:\n",
    "\n",
    "$$\n",
    "\\dfrac{∂}{∂w_0}J(w) = \\dfrac{∂}{∂_{w_0}} \\dfrac{1}{N}\\sum_{i}^N (h(x^{(i)}; w) - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dfrac{∂}{∂{w_0}}J(w) =\\dfrac{2}{N}\\sum_{i}^N (h(x^{(i)}; w) - y^{(i)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{∂}{∂w_1}J(w) = \\frac{∂}{∂w_1} \\frac{1}{N}\\sum_{i}^N (h(x^{(i)}; w) - y^{(i)})^2\n",
    "$$\n",
    "$$\n",
    "\\frac{∂}{∂w_1}J(w) = \\frac{2}{N}\\sum_{i}^N (h(x^{(i)}; w) - y^{(i)}) x^{(i)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e7ba0753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution to 1.4.1: [208.61428561]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\azwad\\onedrive\\documents\\projects\\cs_59866\\venv\\lib\\site-packages\\numpy\\core\\_methods.py:163: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n"
     ]
    }
   ],
   "source": [
    "# Solution to 1.4.1\n",
    "def gradient_of_mean_squared_error(X, y, w):\n",
    "    \"\"\"Calculate the gradient of MSE loss with respect to weights w\n",
    "    \n",
    "    Args:\n",
    "      X: N x d matrix of X values (in this case d=1, we only have 1 feature)\n",
    "      y: N x 1 vector of targets\n",
    "      w: weight vector of length d+1 (in this case d=1)\n",
    "      \n",
    "    Return:\n",
    "      d x 1 gradient vector - consisting of gradient of MSE loss. gradient[j] is\n",
    "          the partial derivative of the MSE with respect to weight j averaged\n",
    "          over all the samples\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    k = X.shape[1]    \n",
    "    predicted_y = linear_regression_predict(X, w)\n",
    "    # Implement this:\n",
    "    gradient = [0, 0]\n",
    "\n",
    "    for i in range(n):\n",
    "        gradient[0] += y[i] - predicted_y[i]\n",
    "        gradient[1] += (y[i] - predicted_y[i])*X[i]\n",
    "    gradient[0] = (gradient[0]*2)/n\n",
    "    gradient[1] = (gradient[1]*2)/n\n",
    "\n",
    "    assert(len(gradient) == 2)\n",
    "    return gradient\n",
    "mean_gradient = gradient_of_mean_squared_error(X_train, y_train, [0, 0])\n",
    "print(f\"Solution to 1.4.1: {np.mean(mean_gradient, axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e3b8b5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution to 1.4.2: Single weight update: [-3626.2538551859056, array([-546.03185699])]\n"
     ]
    }
   ],
   "source": [
    "# Question to 1.4.2\n",
    "def batch_gradient_descent_epoch(X, y, w, alpha):\n",
    "    \"\"\"Returns new weights after one step of batch gradient descent\n",
    "    across the entire dataset.\n",
    "    \n",
    "    Args:\n",
    "      X: N x d matrix (in this case d=1, we only have 1 feature)\n",
    "      y: N x 1 vector\n",
    "      w: weight vector of length d+1 (in this case d=1)\n",
    "      alpha: Floating point learning rate.\n",
    "      \n",
    "    Return:\n",
    "      updated weights as a length-2 vector after 1 step of gradient descent.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    \n",
    "    # Evaluate the error gradient across the entire dataset. NOTE: there are various\n",
    "    # versions of gradient descent that work differently here. In this case we are running\n",
    "    # what is called 'Batch' gradient descent that updates based off the entire training\n",
    "    # dataset on each epoch. Other techniques are mini-batch which break the dataset into\n",
    "    # small chunks and stochastic gradient descent which uses only one sample at a time.\n",
    "    error_gradient = gradient_of_mean_squared_error(X, y, w)\n",
    "    assert(len(error_gradient) == d+1)\n",
    "    return [(w[0] - (alpha * error_gradient[0])), (w[1] - (alpha * error_gradient[1]))]\n",
    "\n",
    "update = batch_gradient_descent_epoch(X_train, y_train, [0, 0], 10)\n",
    "print(f\"Solution to 1.4.2: Single weight update: {update}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fae4e8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution to 1.4.3: Plot:\n"
     ]
    },
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x1a5b3a61090>]"
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Question to 1.4.3\n",
    "from matplotlib.pyplot import figure\n",
    "fig = figure(figsize=(8, 6), dpi=80)\n",
    "alpha = .01\n",
    "weights_gradient_descent = [0, 0]\n",
    "num_epochs = 500\n",
    "errors = []\n",
    "for epoch in range(0, num_epochs):  \n",
    "    # TODO: implement gradient descent loop\n",
    "    # 1. update the weights\n",
    "    predicted_y = linear_regression_predict(X, weights_gradient_descent)\n",
    "    weights_gradient_descent = batch_gradient_descent_epoch(X_train, y_train, weights_gradient_descent, alpha)\n",
    "    #weights_gradient_descent[0] = weights_gradient_descent[0] - ((((y_train[epoch] - predicted_y[epoch])*2)/X_train.shape[0]) * alpha)\n",
    "    #weights_gradient_descent[1] = weights_gradient_descent[1] - ((((y_train[epoch] - predicted_y[epoch]*X_train[epoch])*2)/X_train.shape[0]) * alpha)\n",
    "    # 2. calculate and record the error using the current weights\n",
    "    errors.append(mean_squared_error(y_train, linear_regression_predict(X_train, weights_gradient_descent)))\n",
    "    pass\n",
    "print(\"Solution to 1.4.3: Plot:\")\n",
    "plt.plot(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4e3b6a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1.4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9bdaf5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution to 1.4.5: [array([-3.62288315]), array([-0.55031813])]\n"
     ]
    }
   ],
   "source": [
    "# Question to 1.4.5\n",
    "print(f\"Solution to 1.4.5: {weights_gradient_descent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "74111908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution to 1.4.6: MSE: ****TODO****\n"
     ]
    }
   ],
   "source": [
    "# Question to 1.4.6\n",
    "print(f\"Solution to 1.4.6: MSE: {TODO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f12ffcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1b9f8897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Credit Question 1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de220280",
   "metadata": {},
   "source": [
    "## Problem 2: Implementing Logistic Regression\n",
    "\n",
    "Now we will use logistic regression for classificaton: to predict heart disease using the same dataset as in Homework 1.\n",
    "\n",
    "Gradient descent for logistic regression is very similar to linear regression but not quite the same. \n",
    "\n",
    "Note that in this problem, we are also using 3 different X variables instead of just 1 as in problem 1. Including a column of 1s for the bias term this means `d=4` so our X matrix will be of size `N x 4`, our weight vector will be of length `4`, and error gradient matrix will end up being `N x 4`.\n",
    "\n",
    "\n",
    "2.1: Logistic Regression using Sklearn\n",
    "- 2.1.1: Learn the logistic regression model using a scikit learn package: `sklearn.linear_model import LogisticRegression`. Create an instance and then run `LogisticRegression().fit(...)`. Report the learned weights w=[w_0, w_1, w_2, w_3]. Hint: use classifier.coef_ and classifier.intercept_ to get these out of the trained classifier. \n",
    "- 2.1.2: Report the train log-loss and test log-loss.\n",
    "- 2.1.3: Report the train accuracy and test accuracy using a threshold of 0.5 on the probability prediction to get a classifier.\n",
    "\n",
    "2.2: Gradient Descent for Logistic Regression\n",
    "- 2.2.1: Finish implementation of `gradient_of_log_loss` for logistic regression. Print the average of the gradient for the training data for a weight vector of `w=[0, 0, 0, 0]`. \n",
    "- 2.2.2: Finish implementation of `batch_gradient_descent_logreg_epoch` for logistic regression. Print out the new weights after running this for a single iteration starting from `w=[0, 0, 0, 0]` )\n",
    "- 2.2.3: Run `500 epochs` (steps) of gradient descent starting from `w=[0, 0, 0, 0]` with `alpha=10`. Create a plot where the y-axis is the train log-loss and the x-axis the epoch #.\n",
    "- 2.2.4: Try various values of alpha, num_epochs, and initial values of w (note, see how the error plot changes with alpha. Consider trying various alphas at 10x apart: `[..., 1000, 100, 10, 1, 0.01, 0.001,...]`). Discuss what happens with various choices. What was the best alpha you found? How many epochs should you run? What is a good initialization for w?\n",
    "- 2.2.5: Report learned weights `w` for the best run of gradient descent. \n",
    "- 2.2.6: Report the train mean-square-error and test mean-squared-error for these weights.\n",
    "- 2.2.7: Report the train accuracy and test accuracy using a threshold of 0.5 on the probability prediction to get a classifier.\n",
    "\n",
    "2.3: Derive $\\frac{d \\sigma}{dw}$ where $\\sigma(x;w) = \\frac{1}{1 + e^{-wx}}$\n",
    "\n",
    "2.4: Extra credit: Why did we apply `StandardScaler()` to our varaibles X feature? What happens if we remove it?\n",
    "\n",
    "2.5: Extra credit: We implemented `batch` gradient descent in which each step of gradient descent goes over every single example in the training set. If we wanted to instead implement `mini-batch` in which each step of gradient descent considerd only 50 examples of training data and updated the weights based on that gradient. What would change in our implementation? Discuss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "71b39381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "daec2402",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/heart.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [142]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Load the heart disease dataset:\u001B[39;00m\n\u001B[0;32m      2\u001B[0m columns \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAge\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOldpeak\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMaxHR\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHeartDisease\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m----> 3\u001B[0m heart_disease_dataframe \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m../data/heart.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m[columns]\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Select out X and Y\u001B[39;00m\n\u001B[0;32m      6\u001B[0m X_hd \u001B[38;5;241m=\u001B[39m heart_disease_dataframe[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAge\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOldpeak\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMaxHR\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m.\u001B[39mvalues\n",
      "File \u001B[1;32mc:\\users\\azwad\\onedrive\\documents\\projects\\cs_59866\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    305\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    306\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    307\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39marguments),\n\u001B[0;32m    308\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    309\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mstacklevel,\n\u001B[0;32m    310\u001B[0m     )\n\u001B[1;32m--> 311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\users\\azwad\\onedrive\\documents\\projects\\cs_59866\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    665\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    666\u001B[0m     dialect,\n\u001B[0;32m    667\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    676\u001B[0m     defaults\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[0;32m    677\u001B[0m )\n\u001B[0;32m    678\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 680\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\azwad\\onedrive\\documents\\projects\\cs_59866\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    572\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    574\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 575\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    577\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    578\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32mc:\\users\\azwad\\onedrive\\documents\\projects\\cs_59866\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    930\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    932\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 933\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\azwad\\onedrive\\documents\\projects\\cs_59866\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1213\u001B[0m     mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1214\u001B[0m \u001B[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001B[39;00m\n\u001B[0;32m   1215\u001B[0m \u001B[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001B[39;00m\n\u001B[0;32m   1216\u001B[0m \u001B[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001B[39;00m\n\u001B[1;32m-> 1217\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[call-overload]\u001B[39;49;00m\n\u001B[0;32m   1218\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1219\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1220\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1221\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1222\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1223\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1224\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1225\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1226\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1227\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1228\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32mc:\\users\\azwad\\onedrive\\documents\\projects\\cs_59866\\venv\\lib\\site-packages\\pandas\\io\\common.py:789\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    784\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    785\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    786\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    787\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    788\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 789\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    790\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    791\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    792\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    793\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    794\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    795\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    796\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    797\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    798\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../data/heart.csv'"
     ]
    }
   ],
   "source": [
    "# Load the heart disease dataset:\n",
    "columns = ['Age', 'Oldpeak', 'MaxHR', 'HeartDisease']\n",
    "heart_disease_dataframe = pd.read_csv(\"../../data/heart.csv\")[columns]\n",
    "\n",
    "# Select out X and Y\n",
    "X_hd = heart_disease_dataframe[['Age', 'Oldpeak', 'MaxHR']].values\n",
    "y_hd = heart_disease_dataframe['HeartDisease'].values\n",
    "\n",
    "# Apply standard scaler to X.\n",
    "X_hd = StandardScaler().fit_transform(X_hd)\n",
    "\n",
    "# =======================================================\n",
    "# Our train and test dataset for this problem.\n",
    "# =======================================================\n",
    "X_train_hd, X_test_hd, y_train_hd, y_test_hd = train_test_split(X_hd, y_hd, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be06503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question to 2.1.1\n",
    "print(f\"Solution to 2.1.1: weights learned with sklearn LogisticRegression: {TODO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f401a8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to 2.1.2\n",
    "print(f\"Solution to 2.1.2: Log loss for sklearn LogisticRegression: Train: {TODO}, Test: {TODO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to 2.1.3\n",
    "print(f\"Solution to 2.1.3: Accuracy Train: {TODO}, Accuracy Test: {TODO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3ca92c",
   "metadata": {},
   "source": [
    "### Gradient Descent for Logistic Regression\n",
    "\n",
    "Our hypothesis function for logistic regression is\n",
    "$$h(x; w) = \\frac{1}{1 + e^{-(w_0 + w_1 x_1 + w_2 x_2 + \\dots)}}$$\n",
    "\n",
    "#### Trick: Adding bias term to simplify notation\n",
    "To simplify our notation, we will add a column to x called $x_0$ which always consists of 1s, this makes all our calculations simpler and we can write this in matrix form\n",
    "\n",
    "$$\\frac{1}{1 + e^{-(w_0 x_0 + w_1 x_1 + w_2 x_2 + \\dots)}} :=  \\frac{1}{1 + e^{-w^T  x}}$$\n",
    "\n",
    "#### Vector and Matrix notation\n",
    "\n",
    "Notice we now represent the weights * x as a dot product $w^Tx = w_0 x_0 + w_1x_1 + \\dots$\n",
    "\n",
    "It's always helpful to keep track of dimensions to understand vector/matrix operations:\n",
    "- $w$: is of size $(d x 1)$\n",
    "- $x$ is of size $(d x 1)$\n",
    "- $w^T x$ is a scalar $(1 x 1)$\n",
    "\n",
    "This allows us a nice trick too, we can calculate logistic regression hypothesis function for multiple samples simultaneously using matrix operations. \n",
    "\n",
    "If we introduce a matrix of samples: $X$ is of size $(N x d)$ where each row is a sample and the columns are dimensions, we can then evaluate the entire expression:\n",
    "\n",
    "$X\\cdot w$ is a vector $(N x 1)$\n",
    "\n",
    "$$X\\cdot w = \\begin{pmatrix} \n",
    "w_0 x^{(0)}_0 + w_1x^{(0)}_1 + \\cdots \\\\\n",
    "w_0 x^{(1)}_0 + w_1x^{(1)}_1 + \\cdots \\\\\n",
    "\\cdots \\\\\n",
    "w_0 x^{(N)}_0 + w_1x^{(N)}_1 + \\cdots\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "We can re-write our whole hypothesis function, then, in matrix form:\n",
    "\n",
    "$$h(X;w) :=  \\frac{1}{1 + e^{-X\\cdot w}}$$\n",
    "\n",
    "And in python this is:\n",
    "```def logistic_regression_predict(X, w):\n",
    "    return 1 / (1 + np.exp(-X.dot(w)))```\n",
    "\n",
    "\n",
    "### Deriving Gradient Descent for Logistic Regression\n",
    "\n",
    "Remember, on each update for gradient descent we need to update our weights as follows:\n",
    "$$w_i^{(k+1)}:= w_i^k - \\alpha \\dfrac{∂}{∂w^{k}} J(w^{k})$$\n",
    "\n",
    "The algorithm then iteratively updates all the weights as follows:\n",
    "$$\n",
    "\\forall j: temp_j := w_j - \\alpha \\dfrac{∂}{∂{w_j}} J(w)\\\\\n",
    "\\forall j: w_j := temp_j \\\\\n",
    "$$\n",
    "\n",
    "To implement `gradient_of_log_loss` you will need to implement this gradient for each sample and then `batch_gradient_descent_logreg_epoch` will sum them up. \n",
    "\n",
    "Remember, log loss (also known as binary cross entropy) is defined as follows:\n",
    "\n",
    "$$\n",
    "J(w) = -\\frac{1}{n}\\sum_{i}^n y^{(i)} \\cdot log(h(x^{(i)}; w)) + (1 - y^{(i)}) log(1 - h(x^{(i)}; w))\n",
    "$$\n",
    "\n",
    "The partial gradient turns out to be quite simple using the chain rule if we break up the loss function equation into parts. Note that this uses vector calculus which you may not be familiar with, but it is just using the matrix/vector notation described above.\n",
    "\n",
    "Let:\n",
    "$$g = X\\cdot w$$\n",
    "$$z = \\sigma(g) = \\frac{1}{1 + e^{-g}}$$\n",
    "\n",
    "So that:\n",
    "$$J(w) = -(y \\cdot log(z) + (1 - y) \\cdot log(1 - z))$$\n",
    "\n",
    "Apply the chain rule and you get:\n",
    "$$\n",
    "\\dfrac{∂ J(w)}{∂{w}} = \\dfrac{∂ J(w)}{∂{z}} \\dfrac{∂ z}{∂{g}} \\dfrac{∂ g}{∂{w}} \n",
    "$$\n",
    "\n",
    "The first term is:\n",
    "$$\\dfrac{∂ J(w)}{∂{z}} = -(\\frac{y}{z} - \\frac{1 - y}{1 - z}) = \\frac{z-y}{z(1-z)}$$\n",
    "\n",
    "The derivative of the sigmoid $z$ has a nice simple form:\n",
    "$$\\dfrac{∂ z}{∂{g}} = z\\cdot(1 - z)$$\n",
    "\n",
    "Lastly the derivative of $g$ is just a linear function like LinearRegression, which, as we remember just ends up being the vector of x values.\n",
    "$$\\dfrac{∂ g}{∂{w}} = X$$\n",
    "\n",
    "Putting this all back together, it will simplify to:\n",
    "$$\\dfrac{∂ J(w)}{∂{w}} = X ^T (z - y)$$\n",
    "\n",
    "To help understand: just to check the above, let's look matrix/vector sizes and what they represent:\n",
    "- $X$ is a matrix of size $(N x d)$ where each row is a sample and each column is a dimension of x\n",
    "- $z$ is a vector of size $(N x 1)$ consisting of the model predictions for each samples \n",
    "- $y$ is a vector of size $(N x 1)$ consisting of the true labels for each sample\n",
    "- $(z - y)$ is a vector of $(N x 1)$ consisting of the difference between predictions and true values\n",
    "- $X ^T (z - y)$ is a matrix of size $(d x N)$\n",
    "- $\\dfrac{∂ J(w)}{∂{w}}$ is a matrix of size $(d x 1)$ where each element is:\n",
    "\n",
    "$$\n",
    "\\dfrac{∂J(w)}{∂{w_j}}  = \\frac{1}{N}\\sum_{i}^N x_j^{(i)} (h(x^{(i)}; w) - y^{(i)})\n",
    "$$\n",
    "\n",
    "(Note the division by N to get the average gradient across all samples)\n",
    "\n",
    "\n",
    "Wow! it looks almost identical to linear regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baca73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column of 1s to make things easier. This means we don't need that\n",
    "# hanging w_0 and can evaluate everything at once.\n",
    "X_train_hd_with_1s = np.hstack([np.ones(X_train_hd.shape[0]).reshape(-1,1), X_train_hd])\n",
    "X_test_hd_with_1s = np.hstack([np.ones(X_test_hd.shape[0]).reshape(-1,1), X_test_hd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405000da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question to 2.2.1\n",
    "def logistic_regression_predict(X, w):\n",
    "    \"\"\" Calculate logistic regression for N samples\n",
    "     Args:\n",
    "      X: N x d matrix of X values\n",
    "      w: weight vector of length d\n",
    "      \n",
    "    Return:\n",
    "      N x 1 vector of predictions\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-X.dot(w)))\n",
    "\n",
    "def gradient_of_log_loss(X, y, w):\n",
    "    \"\"\"Calculate the gradient of log loss (aka binary cross entropy) with respect to weights w\n",
    "    \n",
    "    Args:\n",
    "      X: N x d matrix of X values\n",
    "      y: N x 1 vector of targets\n",
    "      w: weight vector of length d\n",
    "      \n",
    "    Return:\n",
    "      d x 1 gradient vector - consisting of gradient of log loss. gradient[j] is\n",
    "          the partial derivative of the loss with respect to weight j averaged over\n",
    "          all N incoming samples\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    predicted_y = logistic_regression_predict(X, w)\n",
    "    error = predicted_y - y\n",
    "    # TODO implement the gradient vector\n",
    "    gradient = TODO\n",
    "    \n",
    "    # Ensure it is the right size!\n",
    "    assert len(gradient) == d\n",
    "    return gradient\n",
    "\n",
    "#mean_gradient = gradient_of_log_loss(X_train_hd_with_1s, y_train_hd, [0, 0, 0, 0])\n",
    "#print(f\"Solution to 2.2.1: {mean_gradient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e654907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question to 2.2.2\n",
    "def batch_gradient_descent_logreg_epoch(X, y, w, alpha):\n",
    "    \"\"\"Returns new weights after one step of batch gradient descent\n",
    "    across the entire dataset.\n",
    "    \n",
    "    Args:\n",
    "      X: N x d matrix \n",
    "      y: N x 1 vector\n",
    "      w: weight vector of length d\n",
    "      alpha: Floating point learning rate.\n",
    "      \n",
    "    Return:\n",
    "      updated weights as a length-d vector after 1 step of gradient descent.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    \n",
    "    # Evaluate the error gradient across the entire dataset. NOTE: there are various\n",
    "    # versions of gradient descent that work differently here. In this case we are running\n",
    "    # what is called 'Batch' gradient descent that updates based off the entire training\n",
    "    # dataset on each epoch. Other techniques are mini-batch which break the dataset into\n",
    "    # small chunks and stochastic gradient descent which uses only one sample at a time.\n",
    "    error_gradient = gradient_of_log_loss(X, y, w)\n",
    "    \n",
    "    # Update our weights based on the gradient update.\n",
    "    new_weights = TODO\n",
    "    \n",
    "    # Make sure it is the right size:\n",
    "    assert len(new_weights) == d\n",
    "    return new_weights\n",
    "\n",
    "#update = batch_gradient_descent_logreg_epoch(X_train_hd_with_1s, y_train_hd, [0, 0, 0, 0], 5)\n",
    "#print(f\"Solution to 2.2.2: Single weight update: {update}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d010da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question to 2.2.3\n",
    "from matplotlib.pyplot import figure\n",
    "fig = figure(figsize=(8, 6), dpi=80)\n",
    "alpha = 5\n",
    "weights_gradient_descent = [0, 0, 0, 0]\n",
    "num_epochs = 500\n",
    "errors = []\n",
    "for epoch in range(0, num_epochs): \n",
    "\n",
    "    # TODO: implement gradient descent loop\n",
    "    # 1. update the weights\n",
    "    # 2. calculate and record the error using the current weights\n",
    "    pass\n",
    "print(\"Solution to 2.2.3: Plot: TODO\")\n",
    "#plt.plot(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c3faf3",
   "metadata": {},
   "source": [
    "Question to 2.2.4\n",
    "\n",
    "TODO: answer to question 2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a57b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question to 2.2.5\n",
    "print(f\"Solution to 2.2.5: {TODO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092e34ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question to 2.2.6\n",
    "print(f\"Solution to 2.2.6: Log loss Train: {TODO}, Test: {TODO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2dba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question to 2.2.7\n",
    "print(f\"Solution to 2.2.7: Accuracy Train: {TODO}, Test: {TODO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe591e4",
   "metadata": {},
   "source": [
    "# Question 2.3\n",
    "2.3: Derive $\\frac{d \\sigma}{dw}$ where $\\sigma(x;w) = \\frac{1}{1 + e^{-wx}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67411e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Credit Question 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbfdc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Credit Question 2.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}