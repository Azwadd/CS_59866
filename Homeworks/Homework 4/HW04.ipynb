{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e9bf62d",
   "metadata": {},
   "source": [
    "Homework 4:\n",
    "---\n",
    "For this homework we will be building various neural networks to classify handwritten digits.\n",
    "\n",
    "### Question 1: Feed-Forward Neural Network to classify MNIST digits.\n",
    "\n",
    "Train a neural network to classify MNIST digits. Remember we have to pick 3 different components to build a neural network: (1) Network Architecture (2) Loss Function and (3) Optimizer. For this assignment use the following:\n",
    "\n",
    "Network Architecture:\n",
    "* [28x28] input \n",
    "* -> Use a view to reshape to [784]\n",
    "* -> Linear Layer [784, Num Hidden Units]\n",
    "* -> Relu\n",
    "* -> Output Linear Layer [NumHiddenUnits, 10]\n",
    "* -> Log Softmax (`torch.nn.functional.log_softmax(output)`)\n",
    "\n",
    "Loss: Please use the negative log likelihood loss function (nll_loss).\n",
    "Optimizer: Please use SGD optimizer (https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD), but you can also experiment with Adam optimizer in 1.6.\n",
    "\n",
    "1.1: Create a Pytorch module to implement a version of this neural network. Display the shape of the network using ` torchsummary.summary`.\n",
    "\n",
    "1.2 Create a training loop to train this model using SGD optimizer. Note, we should train using minibatches. The dataloader will create minibatches for you automatically and you can just loop through as follows:\n",
    "\n",
    "```\n",
    "for epoch in range(0, num_epochs):\n",
    "  # Loop over the dataloader and get minibatches:\n",
    "  for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "    # TODO: Reset gradient on optimizer\n",
    "    # TODO: Evaluate th emodel on data\n",
    "    # TODO: Calculate loss using nll_loss\n",
    "    # TODO: Evaluate gradient with respect to the loss function\n",
    "    # TODO: Run a step of the optimizer.\n",
    "    # TODO: Store the loss function for plotting every X batches.\n",
    "```\n",
    "\n",
    "\n",
    "1.3 Train your model on MNIST digits. Please record the training loss every 100 minibatch steps. Display this as a plot where the x axis is the minibatch index and the y axis is the training loss at that step.\n",
    "\n",
    "1.4: Create a classifier for your model. The output of your model can be evaluated by usuing the forward function, or just calling the model ```trained_nn_model(sample)```. The output of your model will be a tensor of 10 values representing the log softmax value for each class. You can build a classifier by simply taking the index with the highest value. For example if the classifier outputs `[.1, .2, .6, .1]` you would classify it as class 2.\n",
    "\n",
    "1.5: Create and display a confusion matrix for your classifier on your Train and Test data using https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html. Discuss what you observe from this confusion matrix. Which digits are mistaken for each other?\n",
    "\n",
    "1.6: Experiment with different network shapes (number of hidden layers and number of hidden units per layer) or optimizers (for example, you can try Adam instead of SGD). Report the Train and Test accuracy of at least 4 experiments here and discuss what works well and what does not.\n",
    "\n",
    "1.7: Find some examples of images your model misclassifies. Display these images and discuss why the model does not work well on them. \n",
    "\n",
    "### Question 2: Convolutional Neural Network to classify MNIST digits.\n",
    "\n",
    "We will answer similar questions as above but using a convolutional neural network. Please start with this example network module.\n",
    "\n",
    "```\n",
    "class SimpleConvNetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.dense_layer = nn.Linear(320, 50)      \n",
    "        self.output_layer = nn.Linear(50, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.dense_layer(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        output = self.output_layer(x)\n",
    "        return F.log_softmax(output)\n",
    "```\n",
    "\n",
    "2.1: Create a Pytorch module to implement a version of this neural network as implemented above. Display the shape of the network using ` torchsummary.summary`.\n",
    "\n",
    "2.2 Create a training loop to train this model using Adam optimizer. This should be similar to 1.2.\n",
    "\n",
    "2.3 Train your model on MNIST digits. Please record the training loss every 100 minibatch steps. Display this as a plot where the x axis is the minibatch index and the y axis is the training loss at that step.\n",
    "\n",
    "2.4: Create a classifier for your model and report train and test accuracy. How does this compare to the performance of your non-convolutional neural network in question 1?\n",
    "\n",
    "2.5: Create and display a confusion matrix for your classifier on your Train and Test data. Discuss what you observe from this confusion matrix. Which digits are mistaken for each other? How does this differ from 1.5?\n",
    "\n",
    "2.6: Experiment with different kernel sizes and stride parameters to your convolutions. Report the Train and Test accuracy of at least 2 experiments here and discuss what works well and what does not.\n",
    "\n",
    "2.7: Experiment with the network structure. Can you add in an additional convolution? Report the Train and Test accuracy of at least 2 experiments here and discuss what works well and what does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f988c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib import cm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8121903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset:\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "mnist_train_dataset = torchvision.datasets.MNIST(\n",
    "    root='../data/mnist', train=True, download=True, transform=transform) \n",
    "mnist_test_dataset = torchvision.datasets.MNIST(\n",
    "    root='../data/mnist', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008be9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid(data, num_figs):\n",
    "    \"\"\"Function to display a grid of images\n",
    "    Args:\n",
    "      data: torchvision dataset\n",
    "      num_figs: number of images to display\n",
    "    \"\"\"\n",
    "    fig = figure(figsize=(10, 8), dpi=80)\n",
    "    for i in range(num_figs):\n",
    "        fig_i = int(math.sqrt(num_figs))\n",
    "        plt.subplot(fig_i, fig_i, i+1)\n",
    "        plt.tight_layout()\n",
    "        image, label = data[i]\n",
    "        plt.imshow(image.reshape(28, 28), cmap='gray')\n",
    "        plt.title(f\"Label {label}\")\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "image_grid(mnist_train_dataset, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1878a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set the random seed so we will get the seame behavior each run.\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Let's make sure to turn off GPU backend in case you happen to have GPU. The only\n",
    "# reason we do this is not to have to worry about moving data and models on and\n",
    "# off of the GPU for now. If you are training large models and have a GPU, then\n",
    "# you will want to use it.\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb5bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put training and test data into data loaders allow batching of datasets.\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test_dataset, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6da77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self, hidden_units):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "nn_model = SimpleNeuralNet(num_hidden_units)\n",
    "summary(nn_model, (1, 28, 28))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}